# cutile_gpt - Implementation Details

**Pure Tile Programming Philosophy GPU Kernels & Model**

ì´ ë””ë ‰í† ë¦¬ëŠ” cutileGPTì˜ í•µì‹¬ êµ¬í˜„ì„ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ“ êµ¬ì¡°

```
cutile_gpt/
â”œâ”€â”€ model_tile.py              # Pure Tile Philosophy GPT
â”œâ”€â”€ model.py                   # Original CuPy implementation
â”œâ”€â”€ compare.py                 # PyTorch vs cutileGPT comparison
â””â”€â”€ kernels/                   # Tile Programming kernels
    â”œâ”€â”€ layernorm.py          # Declarative LayerNorm
    â”œâ”€â”€ gelu.py               # 8.3x faster GELU
    â”œâ”€â”€ linear.py             # Tile-based matmul
    â”œâ”€â”€ attention.py          # Flash Attention
    â””â”€â”€ ...
```

## ğŸ¯ Tile Programming Kernels

### LayerNorm ([layernorm.py](kernels/layernorm.py))

**ì² í•™**: Declarative normalization - NO manual synchronization

**íŠ¹ì§•**:
- Welford's algorithm for numerical stability
- Two-pass approach: statistics â†’ normalize
- Power-of-2 padding for tile constraints
- Automatic thread management

**ì‚¬ìš©ë²•**:
```python
from cutile_gpt.kernels.layernorm import cutile_layer_norm

x = cp.random.randn(batch, seq, n_embd, dtype=cp.float32)
weight = cp.ones(n_embd, dtype=cp.float32)
bias = cp.zeros(n_embd, dtype=cp.float32)

y = cutile_layer_norm(x, weight, bias)
```

### GELU ([gelu.py](kernels/gelu.py))

**ì„±ëŠ¥**: **8.3x faster** than CuPy! (Verified)

**ì² í•™**: Pure element-wise operations, compiler handles parallelization

**íŠ¹ì§•**:
- GPT-2 style approximation: `0.5 * x * (1 + tanh(...))`
- Automatic vectorization
- No thread management

**ì‚¬ìš©ë²•**:
```python
from cutile_gpt.kernels.gelu import cutile_gelu

x = cp.random.randn(batch, seq, hidden, dtype=cp.float32)
y = cutile_gelu(x)  # 8.3x faster!
```

**ë²¤ì¹˜ë§ˆí¬** (32 Ã— 512 Ã— 768 tensor):
- Tile kernel: 0.600 ms
- CuPy kernel: 4.978 ms
- **Speedup: 8.3x** (Verified on GB10/Blackwell)

### Linear ([linear.py](kernels/linear.py))

**ì² í•™**: Declarative matmul - compiler handles tile operations

**íŠ¹ì§•**:
- Tile-based matrix multiplication
- Automatic Tensor Core dispatch
- Weight transpose caching (28% speedup)
- 2D swizzle pattern for L2 cache locality
- TMA (Tensor Memory Accelerator) on Hopper/Blackwell

**ì‚¬ìš©ë²•**:
```python
from cutile_gpt.kernels.linear import cutile_linear_bias

x = cp.random.randn(batch, seq, in_features, dtype=cp.float32)
weight = cp.random.randn(out_features, in_features, dtype=cp.float32) * 0.02
bias = cp.zeros(out_features, dtype=cp.float32)

y = cutile_linear_bias(x, weight, bias)
```

### Attention ([attention.py](kernels/attention.py))

**ì² í•™**: Flash Attention - O(N) memory, not O(NÂ²)

**íŠ¹ì§•**:
- Online softmax algorithm
- Causal masking support
- Multi-head attention
- TMA for async memory transfers
- NO full attention matrix materialization

**ì‚¬ìš©ë²•**:
```python
from cutile_gpt.kernels.attention import cutile_causal_attention

# Q, K, V: (batch, n_head, seq_len, head_dim)
y = cutile_causal_attention(q, k, v, n_head)
```

## ğŸ¨ Models

### model_tile.py - Pure Tile Philosophy

**ì™„ì „í•œ GPT êµ¬í˜„ with ZERO explicit thread management**

**íŠ¹ì§•**:
- All operations declarative
- Transformer blocks with residual connections
- Text generation support
- minGPT weight loading

**ì‚¬ìš©ë²•**:
```python
from cutile_gpt.model_tile import create_gpt_nano, CutileGPT, GPTConfig

# Quick start
model = create_gpt_nano()

# Forward pass
tokens = cp.array([[100, 200, 300]], dtype=cp.int32)
logits = model.forward(tokens)

# Generate
generated = model.generate(tokens, max_new_tokens=50)

# Custom config
config = GPTConfig(n_layer=6, n_head=4, n_embd=256)
model = CutileGPT(config)
```

**Available configs**:
- `create_gpt_nano()` - 3 layers, 48 dims, 3 heads
- `create_gpt2('gpt2')` - 12 layers, 768 dims, 12 heads
- `create_gpt2('gpt2-medium')` - 24 layers, 1024 dims, 16 heads

### model.py - Original Implementation

**ê¸°ì¡´ CuPy ê¸°ë°˜ êµ¬í˜„ (PyTorch parity ë‹¬ì„±)**

**ì‚¬ìš©ë²•**:
```python
from cutile_gpt.model import CutileGPT, CutileGPTConfig

config = CutileGPTConfig.gpt_tile_medium()
model = CutileGPT(config)

logits, _ = model(idx)
```

## ğŸ”§ ìµœì í™” ê¸°ë²•

### 1. Weight Transpose Caching
ëª¨ë“  weight transposeë¥¼ ì´ˆê¸°í™” ì‹œ pre-compute
- **Impact**: 28% average speedup

### 2. Flash Attention
Online softmaxë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- **Memory**: O(N) instead of O(NÂ²)

### 3. TF32 Tensor Cores
`float32` ì…ë ¥ ìë™ TF32 ë³€í™˜
- **Impact**: 8x faster than FP32 CUDA cores

### 4. 2D Swizzle Pattern
L2 cache locality ìµœì í™”
- Better cache hit rate

### 5. TMA (Tensor Memory Accelerator)
Hopper/Blackwell í•˜ë“œì›¨ì–´ ê°€ì†
- Async memory transfers

## ğŸ“Š ì„±ëŠ¥

### Kernel Level
| Kernel | Tile | CuPy | Speedup |
|--------|------|------|---------|
| GELU (32Ã—512Ã—768) | 0.600 ms | 4.978 ms | **8.3x** (Verified) |
| LayerNorm | Fast | Reference | Competitive |
| Linear | Fast | Reference | Competitive |

### Model Level
| Model | cutileGPT | PyTorch | Result |
|-------|-----------|---------|--------|
| gpt_tile_medium (batch=8, seq=128) | 5.399 ms | 5.174 ms | **Within 4% of PyTorch** |

## ğŸ§ª Testing

```python
# Test individual kernel
python -m cutile_gpt.kernels.gelu

# Test model
python -m cutile_gpt.model_tile

# Compare with PyTorch
python cutile_gpt/compare.py --model nano
```

## ğŸ“š API Reference

### Kernels

**cutile_layer_norm(x, weight, bias, eps=1e-5)**
- Input: `(batch, seq, n_embd)`
- Output: Same shape

**cutile_gelu(x)**
- Input: Any shape
- Output: Same shape
- 8.3x faster than CuPy (Verified)

**cutile_linear_bias(x, weight, bias, weight_t=None)**
- Input: `(..., in_features)`
- Weight: `(out_features, in_features)`
- Output: `(..., out_features)`

**cutile_causal_attention(q, k, v, n_head)**
- Input: `(batch, n_head, seq_len, head_dim)`
- Output: Same shape

### Model

**CutileGPT(config)**
- `forward(idx)` - Forward pass
- `generate(idx, max_new_tokens, temperature, top_k)` - Generate text
- `load_from_mingpt(mingpt_model)` - Load PyTorch weights

## ğŸ“ Tile Philosophy ì›ì¹™

ì´ êµ¬í˜„ì˜ ëª¨ë“  ì»¤ë„ì€ ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤:

1. **Declarative** - WHATì„ ëª…ì‹œ, HOWëŠ” ì»´íŒŒì¼ëŸ¬
2. **No thread IDs** - `ct.bid()` only, no `threadIdx`
3. **No synchronization** - No `__syncthreads()`
4. **High-level ops** - `ct.load()`, `ct.sum()`, `ct.mma()`
5. **Compiler-driven** - Automatic optimization

## ğŸ”— ì°¸ê³ 

- [demo_tile_gpt.py](../demo_tile_gpt.py) - ì™„ì „í•œ ì‹¤í–‰ ì˜ˆì œ
- [TILE_PHILOSOPHY_DEMO.md](../TILE_PHILOSOPHY_DEMO.md) - ì² í•™ ë¬¸ì„œ
- [NVIDIA CUDA Tile Docs](https://docs.nvidia.com/cuda/tile-ir/)

---

**Built with Tile Programming Philosophy** ğŸš€

*Think in WHAT (operations), not HOW (threads)*
