# CUDA Tile MLIR í†µí•© ê°€ëŠ¥ì„± ë¶„ì„

## ğŸ¯ ëª©í‘œ

NVIDIAì˜ ê³µì‹ `cuda-tile` ë ˆí¬ì§€í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„ì§œ Tile ì² í•™ì„ cutileGPTì— ì ìš©í•  ìˆ˜ ìˆëŠ”ì§€ ê²€í† í•©ë‹ˆë‹¤.

---

## ğŸ“¦ cuda-tile ë ˆí¬ì§€í† ë¦¬ êµ¬ì¡°

### ì¶”ê°€ ì™„ë£Œ
```bash
git submodule add https://github.com/NVIDIA/cuda-tile.git external/cuda-tile
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

1. **CUDA Tile Dialect** (MLIR)
   - Tile ê¸°ë°˜ ì—°ì‚°ì˜ first-class operation/type
   - MLIR IR í‘œí˜„

2. **Python Bindings**
   - Pythonì—ì„œ MLIR IR ì¡°ì‘ ê°€ëŠ¥
   - But: ì»¤ë„ ì‘ì„± X, IR ì¡°ì‘ë§Œ

3. **Bytecode System**
   - MLIR â†’ Bytecode â†’ Cubin
   - ì§ì ‘ CUDA Driver APIë¡œ ë¡œë“œ ê°€ëŠ¥

4. **Conformance Tests**
   - ë‹¤ì–‘í•œ MLIR ì˜ˆì œ
   - Operation ì‚¬ìš©ë²• ì°¸ê³ 

---

## ğŸ”§ ì‚¬ì „ ì„¤ì¹˜ ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

```bash
# 1. Build Tools
- CMake 3.20.0+
- C++17 compatible compiler (GCC 9+, Clang 10+)
- Ninja build system

# 2. MLIR/LLVM
- Specific LLVM commit (ìë™ ë‹¤ìš´ë¡œë“œ or ìˆ˜ë™ ë¹Œë“œ)
- MLIR Python bindings (optional)

# 3. CUDA
- CUDA Toolkit 13.1+ (for tileiras compiler)
- Compatible GPU (sm_80+, Ampere/Hopper/Blackwell)
- CUDA Driver API support

# 4. Python
- Python 3.6+ (for bindings)
```

### ë¹Œë“œ ì‹œê°„ ì˜ˆìƒ
```
Automatic LLVM download + build: ~1-2 hours (first time)
CUDA Tile build: ~10-20 minutes
Total: ~1.5-2.5 hours
```

---

## ğŸš€ ë¹Œë“œ ê³¼ì •

### Option 1: Quick Start (ìë™ LLVM ë‹¤ìš´ë¡œë“œ)

```bash
cd external/cuda-tile

# Configure
cmake -G Ninja -S . -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DLLVM_ENABLE_ASSERTIONS=OFF \
  -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON

# Build (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼!)
cmake --build build

# Test
cmake --build build --target check-cuda-tile

# Install tools
cmake --install build --prefix ../../tools/cuda-tile
```

### Option 2: Pre-built LLVM ì‚¬ìš© (ë¹ ë¦„)

```bash
# 1. LLVM ë¯¸ë¦¬ ë¹Œë“œ (í•œ ë²ˆë§Œ)
git clone https://github.com/llvm/llvm-project.git
cd llvm-project
git checkout <compatible-commit>  # cuda-tile/cmake/IncludeLLVM.cmake ì°¸ê³ 

cmake -G Ninja -S llvm -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DLLVM_ENABLE_PROJECTS="mlir" \
  -DLLVM_TARGETS_TO_BUILD="NVPTX;X86"

cmake --build build
cmake --install build --prefix /opt/llvm

# 2. CUDA Tile ë¹Œë“œ (ë¹ ë¦„)
cd ../cuda-tile
cmake -G Ninja -S . -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCUDA_TILE_USE_LLVM_INSTALL_DIR=/opt/llvm \
  -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON

cmake --build build
```

---

## ğŸ“ MLIRë¡œ ì»¤ë„ ì‘ì„±í•˜ê¸°

### ì˜ˆì œ: ê°„ë‹¨í•œ Vector Add

```mlir
// vector_add.mlir
cuda_tile.module @vector_add_module {
    cuda_tile.entry @vector_add(
        %a_ptr: !cuda_tile.tile<!cuda_tile.ptr<f32>>,
        %b_ptr: !cuda_tile.tile<!cuda_tile.ptr<f32>>,
        %c_ptr: !cuda_tile.tile<!cuda_tile.ptr<f32>>
    ) {
        // Load tiles
        %token0 = make_token : !cuda_tile.token
        %a, %token1 = load_ptr_tko weak %a_ptr token=%token0
            : !cuda_tile.tile<!cuda_tile.ptr<f32>> -> !cuda_tile.tile<128xf32>, !cuda_tile.token

        %b, %token2 = load_ptr_tko weak %b_ptr token=%token1
            : !cuda_tile.tile<!cuda_tile.ptr<f32>> -> !cuda_tile.tile<128xf32>, !cuda_tile.token

        // Add tiles
        %c = addf %a, %b : !cuda_tile.tile<128xf32>

        // Store result
        %token3 = store_ptr_tko weak %c_ptr, %c token=%token2
            : !cuda_tile.tile<!cuda_tile.ptr<f32>>, !cuda_tile.tile<128xf32> -> !cuda_tile.token

        return
    }
}
```

### ì˜ˆì œ: Matrix Multiply (ì§„ì§œ Tile ìŠ¤íƒ€ì¼!)

```mlir
// matmul.mlir
cuda_tile.module @matmul_module {
    cuda_tile.entry @matmul(
        %A_ptr: !cuda_tile.tile<!cuda_tile.ptr<f32>>,
        %B_ptr: !cuda_tile.tile<!cuda_tile.ptr<f32>>,
        %C_ptr: !cuda_tile.tile<!cuda_tile.ptr<f32>>,
        %M: !cuda_tile.tile<i32>,
        %N: !cuda_tile.tile<i32>,
        %K: !cuda_tile.tile<i32>
    ) {
        // Constants
        %c0 = constant <i32: 0> : !cuda_tile.tile<i32>
        %c1 = constant <i32: 1> : !cuda_tile.tile<i32>
        %c_tile_size = constant <i32: 32> : !cuda_tile.tile<i32>

        // Initialize accumulator
        %zero_f32 = constant <f32: 0.0> : !cuda_tile.tile<f32>
        %acc_init = broadcast %zero_f32 : !cuda_tile.tile<f32> -> !cuda_tile.tile<32x32xf32>

        // K-dimension loop (reduction)
        %final_acc = for %k_idx in (%c0 to %K, step %c_tile_size) : tile<i32>
            iter_values(%acc = %acc_init) -> (tile<32x32xf32>)
        {
            // Load A tile (32x32)
            %A_tile, %token_a = load_ptr_tko weak %A_ptr
                : !cuda_tile.tile<!cuda_tile.ptr<f32>> -> !cuda_tile.tile<32x32xf32>, !cuda_tile.token

            // Load B tile (32x32)
            %B_tile, %token_b = load_ptr_tko weak %B_ptr token=%token_a
                : !cuda_tile.tile<!cuda_tile.ptr<f32>> -> !cuda_tile.tile<32x32xf32>, !cuda_tile.token

            // Matrix multiply-accumulate
            %new_acc = mmaf %A_tile, %B_tile, %acc
                : tile<32x32xf32>, tile<32x32xf32>, tile<32x32xf32>

            continue %new_acc : tile<32x32xf32>
        }

        // Store result
        %token_out = store_ptr_tko weak %C_ptr, %final_acc
            : !cuda_tile.tile<!cuda_tile.ptr<f32>>, !cuda_tile.tile<32x32xf32> -> !cuda_tile.token

        return
    }
}
```

### ì£¼ëª©í•  ì : ì§„ì§œ ì„ ì–¸ì !

```mlir
// âŒ Python API (í˜„ì¬ cutileGPT)
bid_m, bid_n = swizzle_2d(M, N, tm, tn)  // ìˆ˜ë™ ì¸ë±ì‹±
offs_m = bid_x * TILE_M + ct.arange()    // ìˆ˜ë™ ì˜¤í”„ì…‹

// âœ… MLIR (ì§„ì§œ Tile ìŠ¤íƒ€ì¼)
%final_acc = for %k_idx in (%c0 to %K, step %c_tile_size) : tile<i32>
    iter_values(%acc = %acc_init) -> (tile<32x32xf32>)
{
    // ì»´íŒŒì¼ëŸ¬ê°€ ì•Œì•„ì„œ:
    // - ë¸”ë¡ ì¸ë±ì‹±
    // - ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ
    // - í…ì„œ ì½”ì–´ ë§¤í•‘
}
```

---

## ğŸ”„ ì»´íŒŒì¼ ë° ì‹¤í–‰ í”Œë¡œìš°

### Step 1: MLIR â†’ Bytecode

```bash
cuda-tile-translate vector_add.mlir \
    --bytecode-version=13.1 \
    --mlir-to-cudatilebc \
    --no-implicit-module \
    -o vector_add.tilebc
```

### Step 2: Bytecode â†’ Cubin (AoT compilation)

```bash
# CUDA Toolkitì˜ tileiras ì‚¬ìš©
tileiras --gpu-name sm_100 vector_add.tilebc -o vector_add.cubin
```

ë˜ëŠ” JIT compilation:
```cpp
// Bytecodeë¥¼ ì§ì ‘ ë¡œë“œ (JIT)
cuModuleLoad(&module, "vector_add.tilebc");
```

### Step 3: C++/Pythonì—ì„œ ì‹¤í–‰

```cpp
// C++ (CUDA Driver API)
CUmodule module;
CUfunction kernel;

cuModuleLoad(&module, "vector_add.cubin");
cuModuleGetFunction(&kernel, module, "vector_add");

void* args[] = {&a_ptr, &b_ptr, &c_ptr};
cuLaunchKernel(kernel, 1, 1, 1, 1, 1, 1, 0, stream, args, NULL);
```

```python
# Python (cupy)
import cupy as cp
from cupy.cuda import driver

module = driver.moduleLoad("vector_add.cubin")
kernel = module.get_function("vector_add")

kernel.launch(grid=(1,1,1), block=(1,1,1), args=[a_ptr, b_ptr, c_ptr])
```

---

## âœ… Tile ì² í•™ ì ìš© ê°€ëŠ¥ì„±

### 1. ì§„ì§œ Tile ìŠ¤íƒ€ì¼ ê°€ëŠ¥! âœ…

**MLIRë¡œ ì‘ì„±í•˜ë©´:**
- âœ… ì„ ì–¸ì  í”„ë¡œê·¸ë˜ë°
- âœ… ì»´íŒŒì¼ëŸ¬ ì£¼ë„ ìµœì í™”
- âœ… í•˜ë“œì›¨ì–´ ì¶”ìƒí™”
- âœ… ë¸”ë¡ ì¸ë±ì‹± ìë™
- âœ… Loop-carried values ëª…ì‹œì  (`iter_values`)

**ì˜ˆì‹œ:**
```mlir
// ì„ ì–¸ì : "ë¬´ì—‡ì„" ê³„ì‚°í• ì§€ë§Œ
%result = for %i in (%start to %end, step %step) : tile<i32>
    iter_values(%acc = %init) -> (tile<32x32xf32>)
{
    %a = load_ptr_tko weak %a_ptr : ... -> tile<32x32xf32>, token
    %b = load_ptr_tko weak %b_ptr : ... -> tile<32x32xf32>, token
    %new_acc = mmaf %a, %b, %acc : tile<32x32xf32>, ...
    continue %new_acc : tile<32x32xf32>
}
```

### 2. í†µí•© ë°©ë²•

#### Option A: MLIR ì»¤ë„ + Python í˜¸ìŠ¤íŠ¸ (ì¶”ì²œ) âœ…

```
cutileGPT/
â”œâ”€â”€ kernels_mlir/
â”‚   â”œâ”€â”€ attention.mlir       # MLIRë¡œ ì‘ì„±
â”‚   â”œâ”€â”€ linear.mlir
â”‚   â””â”€â”€ layernorm.mlir
â”œâ”€â”€ kernels_compiled/
â”‚   â”œâ”€â”€ attention.tilebc     # ì»´íŒŒì¼ëœ bytecode
â”‚   â””â”€â”€ attention.cubin      # AoT ì»´íŒŒì¼ (optional)
â””â”€â”€ cutile_gpt/
    â””â”€â”€ model.py             # Pythonì—ì„œ ë¡œë“œ
```

```python
# model.py
import cupy as cp
from cupy.cuda import driver

class CutileGPTMLIR:
    def __init__(self):
        # Load compiled kernels
        self.attention_module = driver.moduleLoad("kernels_compiled/attention.cubin")
        self.linear_module = driver.moduleLoad("kernels_compiled/linear.cubin")

        self.attention_kernel = self.attention_module.get_function("causal_attention")
        self.linear_kernel = self.linear_module.get_function("matmul")

    def forward(self, x):
        # Launch MLIR kernels from Python
        self.attention_kernel.launch(...)
        self.linear_kernel.launch(...)
```

**ì¥ì :**
- âœ… ì§„ì§œ Tile ì² í•™ ì ìš©
- âœ… Python ì¸í„°í˜ì´ìŠ¤ ìœ ì§€
- âœ… êµìœ¡ì  ê°€ì¹˜ ê·¹ëŒ€í™”

**ë‹¨ì :**
- âš ï¸ MLIR í•™ìŠµ í•„ìš”
- âš ï¸ ë¹Œë“œ ë³µì¡ë„ ì¦ê°€
- âš ï¸ ë””ë²„ê¹… ì–´ë ¤ì›€

#### Option B: Hybrid (Python + MLIR) âš ï¸

```python
# ê°„ë‹¨í•œ ê²ƒ: Python API ìœ ì§€
from cutile_gpt.kernels.linear import cutile_linear

# ë³µì¡í•œ ê²ƒ: MLIR ì»¤ë„ ì‚¬ìš©
attention_module = load_mlir_kernel("attention.cubin")
```

**ì¥ì :**
- âœ… ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜
- âœ… ë³µì¡ë„ ë¶„ì‚°

**ë‹¨ì :**
- âš ï¸ ë‘ ì‹œìŠ¤í…œ ìœ ì§€ë³´ìˆ˜

#### Option C: Pure MLIR âŒ ë¹„ì¶”ì²œ

```mlir
// ì „ì²´ë¥¼ MLIRë¡œ ì¬ì‘ì„±
cuda_tile.module @gpt_model { ... }
```

**ì´ìœ :**
- âŒ Python ì¸í„°í˜ì´ìŠ¤ í¬ê¸°
- âŒ ìœ ì—°ì„± ê°ì†Œ
- âŒ ì‹¤ìš©ì„± ë‚®ìŒ

---

## ğŸ“Š í˜„ì‹¤ì  í‰ê°€

### í•  ìˆ˜ ìˆëŠ” ê²ƒ âœ…

1. **MLIR ì»¤ë„ ì‘ì„±**
   - Attention, Linear, LayerNorm
   - ì§„ì§œ ì„ ì–¸ì  ìŠ¤íƒ€ì¼
   - Tile ì² í•™ 100% ì ìš©

2. **ì»´íŒŒì¼ ë° ì‹¤í–‰**
   - MLIR â†’ Bytecode â†’ Cubin
   - Python/C++ì—ì„œ ë¡œë“œ
   - ì„±ëŠ¥ì€ ë¹„ìŠ·í•  ê²ƒ (ê°™ì€ ì»´íŒŒì¼ëŸ¬)

3. **êµìœ¡ì  ê°€ì¹˜**
   - "ì´ê²Œ ì§„ì§œ Tile ìŠ¤íƒ€ì¼ì´ë‹¤" showcase
   - Python API vs MLIR ë¹„êµ

### í•´ì•¼ í•˜ëŠ” ê²ƒ âš ï¸

1. **ë¹Œë“œ ì¸í”„ë¼**
   - LLVM/MLIR ë¹Œë“œ (1-2ì‹œê°„)
   - CUDA Tile ë¹Œë“œ
   - CI/CD í†µí•©

2. **MLIR í•™ìŠµ**
   - Operation semantics
   - Type system
   - Bytecode format

3. **ë””ë²„ê¹… ë„êµ¬**
   - MLIR ë””ë²„ê¹… ì–´ë ¤ì›€
   - Bytecode ê²€ì¦

### ì–»ëŠ” ê²ƒ vs ìƒëŠ” ê²ƒ

**ì–»ëŠ” ê²ƒ:**
- âœ… **ì§„ì§œ Tile ì² í•™** ì ìš©
- âœ… **êµìœ¡ì  ê°€ì¹˜** ê·¹ëŒ€í™”
- âœ… **í”„ë¡œì íŠ¸ ì •ì²´ì„±** ëª…í™•í™”

**ìƒëŠ” ê²ƒ:**
- âš ï¸ **ê°œë°œ ì†ë„** ê°ì†Œ
- âš ï¸ **ìœ ì§€ë³´ìˆ˜ ë³µì¡ë„** ì¦ê°€
- âš ï¸ **ì ‘ê·¼ì„±** ê°ì†Œ (MLIR ì§„ì…ì¥ë²½)

---

## ğŸ’¡ ì¶”ì²œ ë°©í–¥

### Option A: "Dual Implementation" âœ… **ê°•ë ¥ ì¶”ì²œ**

**êµ¬ì¡°:**
```
cutileGPT/
â”œâ”€â”€ cutile_gpt/           # í˜„ì¬ Python êµ¬í˜„ (ìœ ì§€)
â”‚   â””â”€â”€ kernels/
â”‚       â”œâ”€â”€ linear.py     # "Tile API Tutorial"
â”‚       â””â”€â”€ attention.py
â”œâ”€â”€ cutile_gpt_mlir/      # ìƒˆë¡œìš´ MLIR êµ¬í˜„
â”‚   â”œâ”€â”€ kernels/
â”‚   â”‚   â”œâ”€â”€ linear.mlir   # "True Tile Philosophy"
â”‚   â”‚   â””â”€â”€ attention.mlir
â”‚   â””â”€â”€ model_mlir.py     # MLIR ì»¤ë„ ì‚¬ìš©
â””â”€â”€ docs/
    â””â”€â”€ comparison.md     # Python vs MLIR ë¹„êµ
```

**ê°€ì¹˜:**
1. **êµìœ¡ì **: ë‘ ì ‘ê·¼ë²• ë¹„êµ ê°€ëŠ¥
2. **ì‹¤ìš©ì **: Python ë²„ì „ì€ ì‰½ê²Œ ì‚¬ìš©
3. **ì² í•™ì **: MLIR ë²„ì „ì€ ì§„ì§œ Tile ìŠ¤íƒ€ì¼

**README:**
```markdown
## Two Implementations

### 1. Python API (cutile_gpt/)
- ğŸ“ Educational: Learn Tile API usage
- ğŸš€ Practical: Easy to use and modify
- âš ï¸ Note: Low-level optimizations included

### 2. MLIR (cutile_gpt_mlir/)
- ğŸ›ï¸ Philosophical: True Tile-based thinking
- ğŸ“š Advanced: Compiler-driven optimization
- ğŸ¯ Showcase: What Tile IR should be
```

### Option B: "MLIR Only" âš ï¸ ìœ„í—˜

ì „ì²´ë¥¼ MLIRë¡œ ì¬ì‘ì„±
- âŒ ì ‘ê·¼ì„± í¬ê²Œ ê°ì†Œ
- âŒ í˜„ì¬ ì½”ë“œ íê¸°
- âŒ ì‹¤ìš©ì„± í¬ìƒ

### Option C: "í˜„ì¬ ìœ ì§€" ğŸ˜ ì•ˆì „í•˜ì§€ë§Œ ì•„ì‰¬ì›€

Python APIë§Œ ìœ ì§€
- âœ… ì•ˆì „í•˜ê³  ì‹¤ìš©ì 
- âŒ "Tile ì² í•™" ë¬¸ì œ í•´ê²° ì•ˆ ë¨
- âŒ ì •ì²´ì„± ëª¨í˜¸

---

## ğŸ¯ ê²°ë¡  ë° Next Steps

### í•µì‹¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µ

**Q: MLIRë¡œ ì§„ì§œ Tile ì² í•™ ì ìš© ê°€ëŠ¥í•œê°€?**
**A: ì˜ˆ! 100% ê°€ëŠ¥í•©ë‹ˆë‹¤.**

**Q: cutileGPTì— ì ìš©í•  ê°€ì¹˜ê°€ ìˆëŠ”ê°€?**
**A: ì˜ˆ, í•˜ì§€ë§Œ "Dual Implementation" í˜•íƒœë¡œ.**

### ì¶”ì²œ ë¡œë“œë§µ

#### Phase 1: í™˜ê²½ êµ¬ì¶• (1-2ì¼)
```bash
# 1. LLVM/MLIR ë¹Œë“œ
# 2. CUDA Tile ë¹Œë“œ
# 3. ë„êµ¬ ì„¤ì¹˜ í™•ì¸
```

#### Phase 2: ê°„ë‹¨í•œ MLIR ì»¤ë„ (3-5ì¼)
```mlir
# 1. LayerNorm (ê°€ì¥ ê°„ë‹¨)
# 2. Linear (matmul)
# 3. ì„±ëŠ¥ ë¹„êµ with Python version
```

#### Phase 3: Attention êµ¬í˜„ (5-7ì¼)
```mlir
# 1. Flash Attention in MLIR
# 2. ì§„ì§œ ì„ ì–¸ì  ìŠ¤íƒ€ì¼
# 3. êµìœ¡ ìë£Œ ì‘ì„±
```

#### Phase 4: ë¬¸ì„œí™” (3-5ì¼)
```markdown
# 1. Python vs MLIR ë¹„êµ ë¬¸ì„œ
# 2. "True Tile Philosophy" ê°€ì´ë“œ
# 3. MLIR íŠœí† ë¦¬ì–¼
```

### ìµœì¢… ê¶Œì¥ì‚¬í•­

**"Dual Implementation"ìœ¼ë¡œ ê°€ì„¸ìš”!**

1. âœ… í˜„ì¬ Python ì½”ë“œ ìœ ì§€ (ì‹¤ìš©ì„±)
2. âœ… MLIR ë²„ì „ ì¶”ê°€ (ì² í•™)
3. âœ… ë‘˜ì„ ë¹„êµí•˜ëŠ” ë¬¸ì„œ (êµìœ¡)

ì´ë ‡ê²Œ í•˜ë©´:
- **ì‹¤ìš©ì  ê°€ì¹˜** ìœ ì§€
- **êµìœ¡ì  ê°€ì¹˜** ê·¹ëŒ€í™”
- **í”„ë¡œì íŠ¸ ì •ì²´ì„±** ëª…í™•í™”
- **"ì´ê²Œ ì§„ì§œ Tile ìŠ¤íƒ€ì¼"** ì¦ëª…

**ê²°ê³¼:**
cutileGPT = ê°€ì¥ í¬ê´„ì ì¸ CUDA Tile êµìœ¡ ìë£Œ ğŸ“

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [cuda-tile GitHub](https://github.com/NVIDIA/cuda-tile)
- [CUDA Tile IR Specification](https://docs.nvidia.com/cuda/tile-ir/13.1/)
- [MLIR Documentation](https://mlir.llvm.org/)
- cutileGPT Analysis: [CUDA_TILE_PHILOSOPHY_ANALYSIS.md](CUDA_TILE_PHILOSOPHY_ANALYSIS.md)
