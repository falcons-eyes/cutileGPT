# Tile IR ê³ ê¸‰ ê¸°ë²•ì„ í™œìš©í•œ cutileGPT ê°œì„  ë°©ì•ˆ

## ğŸ“š ë¶„ì„ ë°°ê²½

NVIDIA Tile IR ê³µì‹ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ cutileGPTì˜ í˜„ì¬ êµ¬í˜„ê³¼ ë¹„êµí•œ ê²°ê³¼, ì„±ëŠ¥ê³¼ ì½”ë“œ í’ˆì§ˆì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” 5ê°€ì§€ í•µì‹¬ ê°œì„ ì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

**ì°¸ê³  ë¬¸ì„œ:**
- [Tile IR Introduction](https://docs.nvidia.com/cuda/tile-ir/latest/sections/introduction.html)
- [Programming Model](https://docs.nvidia.com/cuda/tile-ir/latest/sections/prog_model.html)
- [Bytecode Operations](https://docs.nvidia.com/cuda/tile-ir/latest/sections/bytecode.html)

---

## ğŸ¯ ê°œì„  ì‚¬í•­ 1: Tensor Views ì‚¬ìš©

### í˜„ì¬ êµ¬í˜„ (linear.py)
```python
# Manual indexing with explicit tuples
a = ct.load(A, index=(bid_m, k), shape=(tm, tk),
            padding_mode=zero_pad, latency=4, allow_tma=True)
```

### ë¬¸ì œì 
- ìˆ˜ë™ ì¸ë±ìŠ¤ ê³„ì‚°ìœ¼ë¡œ ì»´íŒŒì¼ëŸ¬ ìµœì í™” ê¸°íšŒ ì œí•œ
- Shape/stride ì •ë³´ê°€ ë¶„ì‚°ë˜ì–´ ìˆìŒ
- Alignment ê°€ì •ì„ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„ ë¶ˆê°€

### ê°œì„ ì•ˆ: Structured Tensor Views
```python
# Create tensor view with shape and stride information
A_view = ct.make_tensor_view(A, shape=(M, K), strides=(K, 1))
B_view = ct.make_tensor_view(B, shape=(K, N), strides=(N, 1))

# Load with tensor view (compiler can optimize memory access)
a = ct.load_view(A_view, tile_idx=(bid_m, k), tile_shape=(tm, tk),
                 latency=4, allow_tma=True)
```

### ì¥ì 
1. **ì»´íŒŒì¼ëŸ¬ ìµœì í™”**: Shape/stride ì •ë³´ë¡œ ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ ì¶”ë¡  ê°€ëŠ¥
2. **ì½”ë“œ ê°„ê²°ì„±**: Offset ê³„ì‚° boilerplate ì œê±°
3. **Alignment íŒíŠ¸**: `assume` predicateë¡œ alignment ëª…ì‹œ â†’ ë²¡í„°í™” ê°œì„ 
4. **ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ**: ë¬¸ì„œì— ë”°ë¥´ë©´ "superior performance model"

### ì ìš© ëŒ€ìƒ
- `cutile_gpt/kernels/linear.py`: matmul ì»¤ë„ (~10% ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ)
- `cutile_gpt/kernels/attention.py`: Q, K, V ë¡œë”© (~5-10% ê°œì„  ì˜ˆìƒ)

---

## ğŸ¯ ê°œì„  ì‚¬í•­ 2: Partition Viewsë¥¼ í™œìš©í•œ ê³„ì¸µì  íƒ€ì¼ë§

### í˜„ì¬ êµ¬í˜„ (attention.py)
```python
# Single-level tiling with fixed 64x64 tiles
tile_m = 64
tile_n = 64
grid_x = math.ceil(seq_len / tile_m)

# Manual loop over tiles
for j in range(0, Tc):
    k = ct.load(K, index=(batch_idx, head_idx, 0, j), ...)
```

### ë¬¸ì œì 
- ê³ ì •ëœ ë‹¨ì¼ ë ˆë²¨ íƒ€ì¼ë§
- í° í–‰ë ¬ì—ì„œ L2 ìºì‹œ í™œìš© ì œí•œ
- Swizzle íŒ¨í„´ë§Œìœ¼ë¡œëŠ” ë©”ëª¨ë¦¬ ê³„ì¸µ í™œìš© ë¶€ì¡±

### ê°œì„ ì•ˆ: Hierarchical Tiling with Partition Views
```python
# Create partition view for hierarchical tiling
K_partition = ct.create_partition_view(K, outer_tile=(256, 256), inner_tile=(64, 64))

# Automatic hierarchical iteration
for outer_idx in K_partition.outer_tiles():
    # L2 cache-level blocking
    for inner_idx in K_partition.inner_tiles(outer_idx):
        k_tile = ct.load_partition(K_partition, outer_idx, inner_idx)
        # Process inner tile...
```

### ì¥ì 
1. **L2 ìºì‹œ í™œìš©**: Outer tileì´ L2ì— ìœ ì§€ë˜ëŠ” ë™ì•ˆ inner tile ì²˜ë¦¬
2. **ë©”ëª¨ë¦¬ ê³„ì¸µ ìµœì í™”**: GPU ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡°ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ë§¤í•‘
3. **ì»´íŒŒì¼ëŸ¬ ì§€ì›**: Index space ìë™ ê³„ì‚° (`get_index_space_shape`)
4. **í° ì‹œí€€ìŠ¤ ì²˜ë¦¬**: seq_len > 512ì—ì„œ íš¨ê³¼ ê·¹ëŒ€í™”

### ì ìš© ëŒ€ìƒ
- `cutile_gpt/kernels/attention.py`: Flash attention K, V íƒ€ì¼ ì²˜ë¦¬
  - seq_len=512: ~15% ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ
  - seq_len=1024: ~25% ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ

---

## ğŸ¯ ê°œì„  ì‚¬í•­ 3: êµ¬ì¡°í™”ëœ Loop-Carried Variables

### í˜„ì¬ êµ¬í˜„ (attention.py)
```python
# Simple Python loop with manual accumulator management
acc = ct.full((TILE_M, TILE_D), 0.0, dtype=np.float32)
m_i = ct.full((TILE_M, 1), -np.inf, dtype=np.float32)
l_i = ct.full((TILE_M, 1), 0.0, dtype=np.float32)

for j in range(0, Tc):
    # ... computation ...
    acc = acc * alpha  # Manual update
    l_i = l_i * alpha + l_ij
    m_i = m_ij
```

### ë¬¸ì œì 
- Loop-carried dependenciesê°€ ì•”ë¬µì 
- ì»´íŒŒì¼ëŸ¬ê°€ ìµœì í™” ê¸°íšŒ íŒŒì•… ì–´ë ¤ì›€
- Reduction íŒ¨í„´ì´ ëª…ì‹œì ì´ì§€ ì•ŠìŒ

### ê°œì„ ì•ˆ: Structured Loop with Explicit Carry
```python
# Define loop-carried variables explicitly
loop_state = ct.create_loop_state(
    acc=ct.full((TILE_M, TILE_D), 0.0, dtype=np.float32),
    m_i=ct.full((TILE_M, 1), -np.inf, dtype=np.float32),
    l_i=ct.full((TILE_M, 1), 0.0, dtype=np.float32)
)

# Structured loop construct
for j in ct.loop_range(0, Tc, carried_vars=loop_state):
    # ... computation ...

    # Explicit continue with updated state
    loop_state = ct.continue_loop(
        acc=acc * alpha,
        l_i=l_i * alpha + l_ij,
        m_i=m_ij
    )

final_acc, final_m, final_l = loop_state.extract()
```

### ì¥ì 
1. **ëª…ì‹œì  ë°ì´í„° íë¦„**: ì»´íŒŒì¼ëŸ¬ê°€ reduction íŒ¨í„´ ì¸ì‹
2. **ìµœì í™” ê¸°íšŒ**: Loop unrolling, software pipelining ê°€ëŠ¥
3. **ì •í™•ì„±**: Reduction semantics ëª…í™•
4. **Flash Attentionì— ìµœì **: Online softmax reductionì— ì´ìƒì 

### ì ìš© ëŒ€ìƒ
- `cutile_gpt/kernels/attention.py`: Online softmax loop (~5-10% ê°œì„ )
- `cutile_gpt/kernels/linear.py`: K-dimension reduction loop

---

## ğŸ¯ ê°œì„  ì‚¬í•­ 4: í™•ì¥ëœ Optimization Hints

### í˜„ì¬ êµ¬í˜„
```python
@ct.kernel(num_ctas=ct.ByTarget(sm_100=2, sm_120=1, default=1), occupancy=4)
def matmul_kernel(A, B, C, tm, tn, tk):
    # Limited hints
    a = ct.load(A, latency=4, allow_tma=True)
```

### ë¬¸ì œì 
- ì œí•œì ì¸ ìµœì í™” íŒíŠ¸
- Function-level metadata ë¶€ì¡±
- Visibility/kind ëª…ì‹œ ì—†ìŒ

### ê°œì„ ì•ˆ: Comprehensive Optimization Hints
```python
@ct.kernel(
    # Existing hints
    num_ctas=ct.ByTarget(sm_100=2, sm_120=1, default=1),
    occupancy=4,

    # New optimization hints
    optimization_hints={
        'max_register_usage': 128,        # Register pressure control
        'prefer_l1_cache': True,          # L1 vs shared memory trade-off
        'vectorization_factor': 4,        # SIMD width hint
        'unroll_factor': 2,               # Loop unrolling hint
        'pipeline_depth': 3,              # Software pipelining depth
    },

    # Function visibility and kind
    visibility='public',                  # Kernel entry point
    function_kind='device'                # Device-side function
)
def matmul_kernel_optimized(A, B, C, tm, tn, tk):
    # Load with extended hints
    a = ct.load(A,
                latency=4,
                allow_tma=True,
                prefetch_distance=2,      # Prefetch ahead
                cache_policy='streaming') # Streaming vs persistent
```

### ì¥ì 
1. **ì„¸ë°€í•œ ì œì–´**: Register, cache, vectorization ì œì–´
2. **ì»´íŒŒì¼ëŸ¬ ê°€ì´ë“œ**: ìµœì í™” ì „ëµ ëª…ì‹œ
3. **í•˜ë“œì›¨ì–´ íƒ€ê²ŸíŒ…**: GPU ì„¸ëŒ€ë³„ ìµœì í™”
4. **í”„ë¡œíŒŒì¼ë§ ê¸°ë°˜**: Nsight Compute ê²°ê³¼ ë°˜ì˜ ê°€ëŠ¥

### ì ìš© ëŒ€ìƒ
- ëª¨ë“  ì»¤ë„: í”„ë¡œíŒŒì¼ë§ ë°ì´í„° ê¸°ë°˜ íŒíŠ¸ ì¶”ê°€
- íŠ¹íˆ `linear.py`ì˜ matmul: Register spilling ìµœì†Œí™”

---

## ğŸ¯ ê°œì„  ì‚¬í•­ 5: Multi-Dimensional Tensor Operations

### í˜„ì¬ êµ¬í˜„ (attention.py)
```python
# Manual reshape and transpose operations
q = ct.load(Q, ...).reshape((TILE_M, TILE_D))
k = ct.load(K, order=(0, 1, 3, 2), ...).reshape((TILE_D, TILE_N))
```

### ë¬¸ì œì 
- Reshape ì˜¤ë²„í—¤ë“œ (ì‘ì§€ë§Œ ëˆ„ì ë¨)
- Transposeê°€ ì¶”ê°€ ë©”ëª¨ë¦¬ ì ‘ê·¼ ìœ ë°œ ê°€ëŠ¥
- Dimension mappingì´ ëª…ì‹œì ì´ì§€ ì•ŠìŒ

### ê°œì„ ì•ˆ: Native Multi-Dimensional Operations
```python
# Use Tile IR's native dimension operations
Q_view = ct.make_tensor_view(Q, shape=(batch, n_head, seq_len, head_dim))
K_view = ct.make_tensor_view(K, shape=(batch, n_head, seq_len, head_dim))

# Dimension mapping without reshape
q_tile = ct.load_view(Q_view,
                      tile_idx=(batch_idx, head_idx, bid_x, 0),
                      tile_shape=(1, 1, TILE_M, TILE_D),
                      dimension_map=[2, 3])  # Focus on seq_len, head_dim

# Broadcast/iota for dimension generation
offsets = ct.iota(shape=(TILE_M,), dtype=np.int32)
offsets = ct.broadcast(offsets, target_shape=(TILE_M, TILE_N))
```

### ì¥ì 
1. **Zero-Copy**: Reshape ì—†ì´ ì°¨ì› ì¬í•´ì„
2. **ëª…ì‹œì  ì˜ë¯¸**: Dimension mappingì´ ë¶„ëª…
3. **ì»´íŒŒì¼ëŸ¬ ìµœì í™”**: Memory layout ì¶”ë¡  ê°€ëŠ¥
4. **Iota/Broadcast í™œìš©**: ì˜¤í”„ì…‹ ê³„ì‚° íš¨ìœ¨í™”

### ì ìš© ëŒ€ìƒ
- `cutile_gpt/kernels/attention.py`: Q, K, V reshape ì œê±°
- `cutile_gpt/kernels/linear.py`: Input reshape ìµœì í™”

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ì˜í–¥

### ì¢…í•© ê°œì„  íš¨ê³¼ (ëˆ„ì )

| ê°œì„  ì‚¬í•­ | ì˜í–¥ë„ | ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ | ì ìš© ë‚œì´ë„ |
|----------|-------|---------------|-----------|
| **1. Tensor Views** | ë†’ìŒ | 5-10% | ì¤‘ê°„ |
| **2. Partition Views** | ë†’ìŒ (í° seq) | 15-25% (seqâ‰¥512) | ë†’ìŒ |
| **3. Loop-Carried Vars** | ì¤‘ê°„ | 5-10% | ë‚®ìŒ |
| **4. Extended Hints** | ì¤‘ê°„ | 3-7% | ë‚®ìŒ |
| **5. Multi-Dim Ops** | ë‚®ìŒ | 2-5% | ì¤‘ê°„ |
| **ì´í•© (ë¹„ëˆ„ì )** | - | **15-30%** | - |

### ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì˜ˆìƒ íš¨ê³¼

| Seq Length | í˜„ì¬ (ms) | ê°œì„  í›„ (ms) | í–¥ìƒ |
|-----------|----------|------------|------|
| 128 | 1.34 | 1.15 | 14% |
| 256 | 3.21 | 2.68 | 17% |
| 512 | 7.89 | 6.24 | 21% |
| 1024 | 18.34 | 13.76 | 25% |

---

## ğŸ› ï¸ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: ë¹ ë¥¸ íš¨ê³¼ (1-2ì¼)
1. **Loop-Carried Variables** (ê°œì„  3)
   - ë‚œì´ë„: ë‚®ìŒ
   - íš¨ê³¼: 5-10%
   - íŒŒì¼: `attention.py` online softmax loop

2. **Extended Hints** (ê°œì„  4)
   - ë‚œì´ë„: ë‚®ìŒ
   - íš¨ê³¼: 3-7%
   - íŒŒì¼: ëª¨ë“  ì»¤ë„

### Phase 2: ì¤‘ê°„ íš¨ê³¼ (3-5ì¼)
3. **Tensor Views** (ê°œì„  1)
   - ë‚œì´ë„: ì¤‘ê°„
   - íš¨ê³¼: 5-10%
   - íŒŒì¼: `linear.py`, `attention.py`

4. **Multi-Dim Ops** (ê°œì„  5)
   - ë‚œì´ë„: ì¤‘ê°„
   - íš¨ê³¼: 2-5%
   - íŒŒì¼: `attention.py` reshape ì œê±°

### Phase 3: ê³ ê¸‰ ìµœì í™” (5-7ì¼)
5. **Partition Views** (ê°œì„  2)
   - ë‚œì´ë„: ë†’ìŒ
   - íš¨ê³¼: 15-25% (seqâ‰¥512)
   - íŒŒì¼: `attention.py` hierarchical tiling

---

## ğŸ”¬ ê²€ì¦ ê³„íš

### 1. ê¸°ëŠ¥ ê²€ì¦
```python
# ê° ê°œì„  í›„ ì •í™•ì„± í…ŒìŠ¤íŠ¸
pytest cutile_gpt/kernels/test_*.py
python -m cutile_gpt.kernels.linear  # Standalone test
python -m cutile_gpt.kernels.attention
```

### 2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```python
# Before/After ë¹„êµ
python visualize_performance.py  # ì „ì²´ ëª¨ë¸ í”„ë¡œíŒŒì¼ë§
python compare.py                 # PyTorch ëŒ€ë¹„ ë¹„êµ
```

### 3. í”„ë¡œíŒŒì¼ë§
```bash
# Nsight Compute ë¶„ì„
ncu --set full -o profile_improved python visualize_performance.py
ncu --import profile_improved.ncu-rep

# í™•ì¸ í•­ëª©:
# - Memory throughput ê°œì„ 
# - Warp efficiency ì¦ê°€
# - Register spilling ê°ì†Œ
# - L1/L2 cache hit rate í–¥ìƒ
```

---

## ğŸ“ ì½”ë“œ ì˜ˆì‹œ: Tensor Views ì ìš©

### Before (í˜„ì¬)
```python
@ct.kernel(num_ctas=ct.ByTarget(sm_100=2, sm_120=1, default=1), occupancy=4)
def matmul_kernel(A, B, C, tm: ConstInt, tn: ConstInt, tk: ConstInt):
    M = A.shape[0]
    N = B.shape[1]

    bid_m, bid_n = swizzle_2d(M, N, tm, tn)
    num_tiles_k = ct.num_tiles(A, axis=1, shape=(tm, tk))

    acc = ct.full((tm, tn), 0, dtype=ct.float32)
    zero_pad = ct.PaddingMode.ZERO

    for k in range(num_tiles_k):
        a = ct.load(A, index=(bid_m, k), shape=(tm, tk),
                    padding_mode=zero_pad, latency=4, allow_tma=True)
        b = ct.load(B, index=(k, bid_n), shape=(tk, tn),
                    padding_mode=zero_pad, latency=4, allow_tma=True)
        acc = ct.mma(a, b, acc)

    ct.store(C, index=(bid_m, bid_n), tile=acc.astype(C.dtype))
```

### After (Tensor Views ì ìš©)
```python
@ct.kernel(
    num_ctas=ct.ByTarget(sm_100=2, sm_120=1, default=1),
    occupancy=4,
    optimization_hints={
        'max_register_usage': 128,
        'prefer_l1_cache': True,
        'vectorization_factor': 4,
    }
)
def matmul_kernel_v2(A, B, C, tm: ConstInt, tn: ConstInt, tk: ConstInt):
    M = A.shape[0]
    N = B.shape[1]
    K = A.shape[1]

    # Create tensor views with shape/stride information
    A_view = ct.make_tensor_view(A, shape=(M, K), strides=(K, 1))
    B_view = ct.make_tensor_view(B, shape=(K, N), strides=(N, 1))
    C_view = ct.make_tensor_view(C, shape=(M, N), strides=(N, 1))

    # Compiler can optimize based on alignment
    ct.assume(A_view.is_aligned(16))
    ct.assume(B_view.is_aligned(16))

    bid_m, bid_n = swizzle_2d(M, N, tm, tn)
    num_tiles_k = ct.cdiv(K, tk)

    # Structured loop with explicit carry
    loop_state = ct.create_loop_state(
        acc=ct.full((tm, tn), 0, dtype=ct.float32)
    )

    for k in ct.loop_range(num_tiles_k, carried_vars=loop_state):
        # Load with tensor views (compiler optimizes access pattern)
        a = ct.load_view(A_view,
                        tile_idx=(bid_m, k),
                        tile_shape=(tm, tk),
                        latency=4,
                        allow_tma=True,
                        prefetch_distance=2)

        b = ct.load_view(B_view,
                        tile_idx=(k, bid_n),
                        tile_shape=(tk, tn),
                        latency=4,
                        allow_tma=True,
                        prefetch_distance=2)

        # Update accumulator
        new_acc = ct.mma(a, b, loop_state.acc)
        loop_state = ct.continue_loop(acc=new_acc)

    final_acc = loop_state.extract().acc

    # Store with tensor view
    ct.store_view(C_view,
                  tile_idx=(bid_m, bid_n),
                  tile=final_acc.astype(C.dtype))
```

---

## ğŸ“ í•™ìŠµ í¬ì¸íŠ¸

### Tile IRì˜ ì² í•™
1. **ì¶”ìƒí™” ê³„ì¸µ**: SIMT ëŒ€ì‹  tile-based ì‚¬ê³ 
2. **ì»´íŒŒì¼ëŸ¬ ì‹ ë¢°**: ëª…ì‹œì  ì •ë³´ ì œê³µ â†’ ì»´íŒŒì¼ëŸ¬ ìµœì í™”
3. **ì„±ëŠ¥ í¬í„°ë¹Œë¦¬í‹°**: GPU ì„¸ëŒ€ ê°„ ì´ì‹ì„± ìœ ì§€í•˜ë©° ì„±ëŠ¥ í™•ë³´

### ì™œ ì´ëŸ° ê¸°ë²•ì´ ì¤‘ìš”í•œê°€?
- **cuBLASëŠ” ì´ë¯¸ ì™„ì„±**: ì§ì ‘ ì‘ì„±í•œ ì»¤ë„ì´ cuBLASë¥¼ ì´ê¸°ê¸° ì–´ë ¤ì›€
- **íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ìµœì í™”**: Flash Attentionì²˜ëŸ¼ íŠ¹ìˆ˜í•œ íŒ¨í„´ì— ê°•ì 
- **êµìœ¡ì  ê°€ì¹˜**: GPU ì•„í‚¤í…ì²˜ ì´í•´ì™€ ìµœì í™” ê¸°ë²• í•™ìŠµ

### cutileGPTì˜ ë°©í–¥ì„±
í˜„ì¬ cutileGPTëŠ” **PyTorch parity (1.01x faster)**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
ì´ ê°œì„ ì•ˆë“¤ì„ ì ìš©í•˜ë©´:

1. **seq_len â‰¤ 256**: 10-15% ì¶”ê°€ í–¥ìƒ â†’ **1.15x faster**
2. **seq_len â‰¥ 512**: 20-30% ì¶”ê°€ í–¥ìƒ â†’ **1.30x faster**
3. **êµìœ¡ì  ê°€ì¹˜**: Tile IR ê³ ê¸‰ ê¸°ë²• showcase

---

## âœ… Next Steps

1. **Phase 1 êµ¬í˜„**: Loop-carried variables + Extended hints
2. **ë²¤ì¹˜ë§ˆí¬**: ê°œì„  ì „í›„ ë¹„êµ
3. **Phase 2 êµ¬í˜„**: Tensor views + Multi-dim ops
4. **Phase 3 êµ¬í˜„**: Partition views (í° ì‹œí€€ìŠ¤ìš©)
5. **ë¬¸ì„œí™”**: ê° ê¸°ë²•ì˜ ì ìš© ì‚¬ë¡€ ì •ë¦¬

---

## ğŸ“š References

- [NVIDIA Tile IR Documentation](https://docs.nvidia.com/cuda/tile-ir/latest/)
- [Flash Attention Paper](https://arxiv.org/abs/2205.14135) - Online softmax ê¸°ë²•
- [Tensor Core Programming Guide](https://docs.nvidia.com/cuda/parallel-thread-execution/#tensor-cores) - mma ìµœì í™”
- cutileGPT Current Performance: [OPTIMIZATION_SUMMARY.md](OPTIMIZATION_SUMMARY.md)
