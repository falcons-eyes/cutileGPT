# cutileGPT

> **Pure Tile Programming Philosophy: Think in WHAT, not HOW**

A complete GPT implementation that proves **declarative GPU programming** works. Using NVIDIA's CUDA Tile framework, cutileGPT achieves **41x speedup on kernels** and **matches PyTorch on full models** - all with **~10MB footprint** vs PyTorch's ~2GB.

**ğŸ¯ Core Philosophy**: Specify WHAT you want (operations), let the compiler handle HOW (threads, sync, memory).

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![CUDA](https://img.shields.io/badge/CUDA-13.0%2B-76b900.svg)](https://developer.nvidia.com/cuda-toolkit)
[![Python](https://img.shields.io/badge/Python-3.13%2B-3776ab.svg)](https://www.python.org/)

---

## ğŸ¨ Tile Programming Philosophy

### The Paradigm Shift

```python
# âŒ Traditional CUDA (Imperative HOW)
@cuda.jit
def kernel(x, y, N):
    tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x
    __shared__ smem[256]
    smem[threadIdx.x] = x[tid]
    __syncthreads()
    # ... manual reduction loops ...

# âœ… Tile Programming (Declarative WHAT)
@ct.kernel
def kernel(X, Y, N):
    x_tile = ct.load(X, ...)      # "Load this data"
    mean = ct.sum(x_tile) / N     # "Compute mean"
    ct.store(Y, ...)              # "Store result"
    # Compiler handles threads, sync, and optimization!
```

### Why This Matters

| Traditional CUDA | **Tile Programming** |
|-----------------|---------------------|
| âŒ Manual thread management | âœ… **Compiler handles** |
| âŒ Explicit `__syncthreads()` | âœ… **Auto dependencies** |
| âŒ ~150 lines/kernel | âœ… **~20 lines** |
| âŒ GPU-specific code | âœ… **Hardware portable** |
| âŒ Manual optimization | âœ… **Compiler-driven** |

### Real Results

- **GELU kernel**: **41.21x faster** than CuPy (0.627ms vs 25.855ms)
- **Full GPT model**: **1.01x faster** than PyTorch (5.175ms vs 5.209ms)
- **Code reduction**: **87% less code** (150 lines â†’ 20 lines)
- **Dependency size**: **200x smaller** (~10MB vs ~2GB)

**Try it yourself**: `uv run python demo_tile_gpt.py`

---

## ğŸš€ Performance Results

**cutileGPT now matches PyTorch performance!**

### AS-IS vs TO-BE Comparison

Tested on NVIDIA GB10 (Blackwell, SM_121):

| Implementation | Latency (ms) | Throughput (tok/s) | Result |
|----------------|--------------|-------------------|---------|
| **PyTorch minGPT (AS-IS)** | 5.209 | 196.2K | Baseline |
| **cutileGPT (TO-BE)** | 5.175 | 197.1K | âœ… **1.01x faster** |

*Workload: tile-medium (6 layers, 128 dims), batch=8, seq=128, vocab=50257*

### Detailed Performance by Model Size

| Model | Layers | Dims | Latency (ms) | Throughput (tok/s) |
|-------|--------|------|--------------|-------------------|
| **gpt_nano** | 3 | 48 | 1.03 | 248K |
| **gpt_tile_small** | 4 | 64 | 0.96 | 268K |
| **gpt_micro** | 4 | 128 | 1.14 | 225K |
| **gpt_tile_medium** | 6 | 128 | 1.34 | 191K |
| **gpt_mini** | 6 | 192 | 2.67 | 96K |
| **gpt_tile_large** | 8 | 256 | 2.63 | 97K |

*Workload: batch=4, seq=64, 100 iterations*

---

## ğŸ“Š Interactive Performance Dashboard

View detailed profiling results with interactive charts:

**[ğŸ“ˆ Open Performance Dashboard](profiling_results/performance_dashboard.html)**

The dashboard includes:
- Forward pass latency comparison across all model configs
- Throughput analysis (tokens/sec)
- Latency percentile distribution (min, median, p95, p99, max)
- Model architecture breakdown
- Efficiency metrics (throughput per layer)

Raw profiling data: [`profiling_results/profiling_data.json`](profiling_results/profiling_data.json)

---

## ğŸ¯ Why cutileGPT?

### Tile Programming Benefits

**ğŸ§  Developer Productivity**
- âœ… **87% less code** - 20 lines vs 150 lines per kernel
- âœ… **Declarative** - Focus on WHAT, not HOW
- âœ… **No manual sync** - Compiler manages dependencies
- âœ… **Fewer bugs** - No thread indexing errors

**ğŸš€ Performance**
- âœ… **41x faster GELU** - Compiler-optimized math
- âœ… **Matches PyTorch** - 1.01x on full model
- âœ… **Auto-tuning** - Optimal for each GPU
- âœ… **Flash Attention** - O(N) memory, not O(NÂ²)

**ğŸ“¦ Deployment**
- âœ… **200x smaller** - ~10MB vs PyTorch's ~2GB
- âœ… **Edge-ready** - Embedded devices
- âœ… **Serverless** - Lambda-compatible
- âœ… **Fast builds** - Docker-friendly

**ğŸ”® Future-Proof**
- âœ… **Hardware portable** - Same code, different GPUs
- âœ… **Compiler updates** - Free performance improvements
- âœ… **No vendor lock-in** - Standard tile operations
- âœ… **Educational** - Learn modern GPU programming

---

## ğŸ”§ Key Optimizations

### 1. **Weight Transpose Caching** ğŸš€

Pre-compute and cache all weight transposes during initialization:

```python
def _precompute_transposes(self):
    """Cache weight transposes to avoid runtime overhead."""
    for key, weight in self.weights.items():
        if 'weight' in key and weight.ndim == 2:
            weight_t = cp.transpose(weight)
            self.weight_transposes[key] = cp.ascontiguousarray(weight_t)
```

**Impact**: 28% average speedup, up to 33% on large models

### 2. **Removed Fused MLP** âŒ

The fused MLP kernel degraded performance by **14x** on realistic workloads. Reverted to simple, proven approach:

```python
# Before (Fused MLP): 14x slower
mlp_out = cutile_fused_mlp(x, w_fc, b_fc, w_proj, b_proj)

# After (Separate ops): Fast and correct
hidden = cutile_linear_bias(x, w_fc, b_fc, w_fc_t)
hidden = cutile_gelu(hidden)
mlp_out = cutile_linear_bias(hidden, w_proj, b_proj, w_proj_t)
```

**Lesson**: cuBLAS-optimized matmul is incredibly hard to beat. Simple solutions often win.

### 3. **Flash Attention**

Online softmax algorithm eliminates full attention matrix materialization:

```python
# Single-pass attention with O(N) memory instead of O(NÂ²)
for j in range(Tc):
    qk = ct.mma(q, k_tile, qk)  # Compute QK^T tile
    p = ct.exp2(qk * scale - m_ij)  # Online softmax
    acc = ct.mma(p, v_tile, acc)  # Accumulate weighted values
```

### 4. **TF32 Tensor Cores**

Automatic TF32 acceleration for `float32` inputs on Ampere+:

```python
dtype = ct.tfloat32 if A.dtype == ct.float32 else A.dtype
a = ct.load(A, ...).astype(dtype)  # Auto-converts to TF32
```

**Impact**: 8x faster than FP32 CUDA cores with minimal accuracy loss

### 5. **2D Swizzle Pattern**

Optimizes L2 cache locality for matrix multiplication:

```python
def swizzle_2d(M, N, tm, tn):
    """Reorder blocks for better cache hits."""
    bid = ct.bid(0)
    group_id = bid // (GROUP_SIZE_M * num_bid_n)
    bid_m = first_bid_m + (bid % group_size_m)
    return bid_m, bid_n
```

### 6. **TMA (Tensor Memory Accelerator)**

Hardware-accelerated async memory transfers on Hopper/Blackwell:

```python
a = ct.load(A, index=(bid_m, k), shape=(tm, tk),
            padding_mode=zero_pad, latency=4, allow_tma=True)
```

---

## ğŸ“¦ Architecture

cutileGPT is built on a clean 3-layer architecture with **zero PyTorch dependencies**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Application Layer                 â”‚
â”‚   (Pure CuPy model implementation)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Array Management (CuPy)           â”‚
â”‚   (NumPy-compatible GPU arrays)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   GPU Kernels (NVIDIA cutile)       â”‚
â”‚   (Custom CUDA tile-based kernels)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Hardware Layer                    â”‚
â”‚   (Hopper SM_100, Blackwell SM_120+)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
cutileGPT/
â”œâ”€â”€ cutile_gpt/                      # ğŸ¯ Tile Programming Implementation
â”‚   â”œâ”€â”€ model_tile.py                # Pure Tile Philosophy GPT model
â”‚   â”œâ”€â”€ model.py                     # Original CuPy-based model
â”‚   â””â”€â”€ kernels/                     # Declarative Tile Kernels
â”‚       â”œâ”€â”€ layernorm.py             # âœ… Declarative normalization
â”‚       â”œâ”€â”€ gelu.py                  # âœ… 41x faster activation
â”‚       â”œâ”€â”€ linear.py                # âœ… Tile-based matmul
â”‚       â”œâ”€â”€ attention.py             # âœ… Flash Attention
â”‚       â””â”€â”€ embedding.py             # Embedding lookup
â”‚
â”œâ”€â”€ demo_tile_gpt.py                 # ğŸ® Complete Tile Philosophy Demo
â”œâ”€â”€ TILE_PHILOSOPHY_DEMO.md          # ğŸ“– Philosophy documentation
â”œâ”€â”€ ARCHITECTURE_VISION.md           # ğŸ—ï¸ Project vision & roadmap
â”œâ”€â”€ CUTILE_PYTHON_PHILOSOPHY_ANALYSIS.md  # ğŸ”¬ Philosophy analysis
â”‚
â”œâ”€â”€ profiling_results/               # Performance data
â”‚   â”œâ”€â”€ performance_dashboard.html   # Interactive dashboard
â”‚   â””â”€â”€ profiling_data.json          # Raw benchmark data
â”‚
â”œâ”€â”€ mlir_research/                   # ğŸ§ª Optional MLIR backend research
â”‚   â”œâ”€â”€ README.md                    # Research overview
â”‚   â”œâ”€â”€ cutile_gpt_mlir/             # MLIR kernel experiments
â”‚   â””â”€â”€ LLVM_MLIR_BUILD_SOLUTION.md  # Build documentation
â”‚
â”œâ”€â”€ external/                        # Git submodules
â”‚   â”œâ”€â”€ cutile-python/               # NVIDIA CUDA Tile framework
â”‚   â””â”€â”€ minGPT/                      # Reference implementation
â”‚
â”œâ”€â”€ visualize_performance.py         # Performance visualization
â”œâ”€â”€ test_text_generation.py          # Text generation demo
â””â”€â”€ README.md                        # This file
```

**Key Files**:
- ğŸ® [demo_tile_gpt.py](demo_tile_gpt.py) - **Start here!** Complete Tile Philosophy demo
- ğŸ¯ [model_tile.py](cutile_gpt/model_tile.py) - Pure Tile Philosophy GPT model
- ğŸ“– [TILE_PHILOSOPHY_DEMO.md](TILE_PHILOSOPHY_DEMO.md) - Philosophy docs & results
- ğŸ—ï¸ [ARCHITECTURE_VISION.md](ARCHITECTURE_VISION.md) - Project vision
- ğŸ“ [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) - Detailed directory structure

---

## âš¡ Quick Start

### Try Tile Philosophy Demo

```bash
# Clone and install
git clone --recursive https://github.com/falcons-eyes/cutileGPT.git
cd cutileGPT
uv sync

# Run complete demo (all tests pass!)
uv run python demo_tile_gpt.py
```

**What you'll see**:
```
âœ… Part 1: Individual Tile kernels (LayerNorm, GELU, Linear, Attention)
âœ… Part 2: Transformer block test
âœ… Part 3: Complete GPT model (forward + generation)
âœ… Part 4: Philosophy comparison (Traditional vs Tile)
âœ… Part 5: Performance benchmark (41x speedup!)

SUCCESS: All Tests Passed!
```

### Use Individual Kernels

```python
import cupy as cp
from cutile_gpt.kernels.layernorm import cutile_layer_norm
from cutile_gpt.kernels.gelu import cutile_gelu
from cutile_gpt.kernels.linear import cutile_linear_bias

# LayerNorm - Declarative
x = cp.random.randn(4, 128, 768, dtype=cp.float32)
weight = cp.ones(768, dtype=cp.float32)
bias = cp.zeros(768, dtype=cp.float32)
y = cutile_layer_norm(x, weight, bias)  # No threads, no sync!

# GELU - 41x faster than CuPy!
y = cutile_gelu(x)

# Linear - Tile-based matmul
y = cutile_linear_bias(x, weight, bias)
```

### Use Complete GPT Model

```python
from cutile_gpt.model_tile import create_gpt_nano

# Create model (pure Tile Philosophy)
model = create_gpt_nano()

# Forward pass
tokens = cp.array([[100, 200, 300]], dtype=cp.int32)
logits = model.forward(tokens)  # (1, 3, 50257)

# Generate text
generated = model.generate(tokens, max_new_tokens=50)
```

---

## ğŸš€ Setup

### Prerequisites

- **Python 3.13+**
- **CUDA 13.0+**
- **NVIDIA GPU** with compute capability 10.0+ (Hopper) or 12.0+ (Blackwell)
  - Tested on: GB10 (Blackwell, SM_121)

### Installation

```bash
# Clone with submodules
git clone --recursive https://github.com/falcons-eyes/cutileGPT.git
cd cutileGPT

# Or if already cloned, initialize submodules
git submodule update --init --recursive

# Install dependencies with uv
uv sync
```

**Note**: PyTorch is **NOT** required for cutileGPT inference. It's only used in `compare.py` for benchmarking against the reference implementation.

---

## ğŸ’» Usage

### Quick Start (CuPy-only inference)

```python
import cupy as cp
from cutile_gpt.model import CutileGPT, CutileGPTConfig

# Create a tile-optimized model (power-of-2 dimensions)
config = CutileGPTConfig.gpt_tile_small()
model = CutileGPT(config)

# Create input token indices (CuPy array)
batch_size, seq_len = 1, 16
idx = cp.random.randint(0, config.vocab_size, (batch_size, seq_len), dtype=cp.int64)

# Forward pass
logits, _ = model(idx)
print(f"Logits shape: {logits.shape}")  # (1, 16, 50257)

# Autoregressive generation
generated = model.generate(
    idx[:, :5],           # Start with 5 tokens
    max_new_tokens=20,    # Generate 20 more
    temperature=0.8,
    top_k=40
)
print(f"Generated: {cp.asnumpy(generated[0]).tolist()}")
```

### Text Generation with GPT-2 Tokenizer

```python
import cupy as cp
from transformers import GPT2Tokenizer
from cutile_gpt.model import CutileGPT, CutileGPTConfig

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

# Create model
config = CutileGPTConfig.gpt2()
model = CutileGPT(config)

# Tokenize prompt
prompt = "The future of artificial intelligence is"
encoded = tokenizer.encode(prompt, return_tensors='pt')
idx = cp.array([encoded[0].tolist()], dtype=cp.int64)

# Generate
generated = model.generate(idx, max_new_tokens=50, temperature=0.9, top_k=50)

# Decode
output_text = tokenizer.decode(cp.asnumpy(generated[0]).tolist())
print(output_text)
```

### Load Weights from minGPT (PyTorch)

```python
import torch
from mingpt.model import GPT as minGPT
from cutile_gpt.model import CutileGPT, CutileGPTConfig

# Load pretrained minGPT model
mingpt_model = minGPT.from_pretrained('gpt2')
mingpt_model.eval()

# Create cutileGPT with matching config
config = CutileGPTConfig.gpt2()
cutile_model = CutileGPT(config)

# Transfer weights (PyTorch â†’ CuPy via NumPy)
cutile_model.load_from_mingpt(mingpt_model)

# Now use CuPy for inference
import cupy as cp
idx = cp.array([[1, 2, 3, 4, 5]], dtype=cp.int64)
logits, _ = cutile_model(idx)
```

### Run Correctness Test

Verify numerical accuracy against PyTorch minGPT:

```bash
uv run python cutile_gpt/compare.py --model nano
```

### Run Performance Benchmarks

```bash
# Compare AS-IS (PyTorch) vs TO-BE (cutileGPT)
uv run python cutile_gpt/compare.py --benchmark --model tile-medium --batch-size 8 --seq-len 128 --vocab-size 50257

# Generate interactive performance dashboard
uv run python visualize_performance.py --run-profiling

# Profile with NVIDIA Nsight Systems
bash scripts/run_nsys_profile.sh
```

---

## ğŸ“‹ Available Model Configurations

| Config | Layers | Heads | Embedding Dim | Block Size | Description |
|--------|--------|-------|---------------|------------|-------------|
| `gpt_nano` | 3 | 3 | 48 | 128 | Minimal test config |
| `gpt_micro` | 4 | 4 | 128 | 256 | Small test config |
| `gpt_mini` | 6 | 6 | 192 | 256 | Medium test config |
| `gpt2` | 12 | 12 | 768 | 1024 | GPT-2 (124M params) |
| **`gpt_tile_small`** | 4 | 4 | **64** | 128 | **Power-of-2 optimized** |
| **`gpt_tile_medium`** | 6 | 4 | **128** | 256 | **Power-of-2 optimized** |
| **`gpt_tile_large`** | 8 | 8 | **256** | 512 | **Power-of-2 optimized** |

**Note**: Tile-optimized configs (`gpt_tile_*`) use power-of-2 embedding dimensions, eliminating tile padding overhead and maximizing GPU utilization.

---

## ğŸ“Š Profiling & Visualization

### Generate Performance Dashboard

```bash
# Run comprehensive profiling
uv run python visualize_performance.py --run-profiling

# Open the interactive dashboard
open profiling_results/performance_dashboard.html
```

### Custom Profiling

```bash
# Custom batch size and sequence length
uv run python visualize_performance.py --run-profiling --batch-size 16 --seq-len 256

# Profile with NVIDIA Nsight Systems
bash scripts/run_nsys_profile.sh

# View nsys results
nsys-ui profiling_results/cutile_nsys.nsys-rep
```

### Profiling Data Format

The `profiling_data.json` contains structured benchmark results:

```json
{
  "metadata": {
    "timestamp": "2026-01-26T14:56:21.406610",
    "gpu_name": "NVIDIA GB10",
    "compute_capability": "12.1",
    "cuda_version": "13000"
  },
  "benchmarks": {
    "gpt_tile_medium": {
      "config": {"n_layer": 6, "n_head": 4, "n_embd": 128},
      "timing": {
        "mean": 1.34, "std": 0.10, "min": 1.19, "max": 1.84,
        "median": 1.34, "p95": 1.51, "p99": 1.82
      },
      "throughput_tokens_per_sec": 191079
    }
  }
}
```

---

## ğŸ”¬ Technical Deep Dive

### What is Tile Programming?

Tile Programming is a **declarative approach** to GPU programming where you:
1. **Specify WHAT** operations you want (load, reduce, multiply)
2. **Let compiler decide HOW** to execute (threads, sync, memory)
3. **Achieve better performance** through compiler optimization

**Example: LayerNorm**

```python
# Traditional CUDA: ~150 lines
# - Manual thread indexing (threadIdx.x, blockIdx.x)
# - Explicit shared memory (__shared__ float smem[256])
# - Manual reduction loops (for s = 128; s > 0; s >>= 1)
# - Multiple __syncthreads() calls
# - Error-prone bounds checking

# Tile Programming: ~20 lines
@ct.kernel
def layernorm_kernel(X, W, B, Y, eps, N):
    bid = ct.bid(0)  # Block ID only, NO thread IDs!

    # Load tile - compiler handles threading
    x = ct.load(X, index=(bid, 0), shape=(1, TILE_N))

    # Compute mean/variance - compiler handles reduction
    mean = ct.sum(x) / N
    var = ct.sum(x * x) / N - mean * mean

    # Normalize - compiler handles broadcasting
    x_norm = (x - mean) / ct.sqrt(var + eps)
    y = x_norm * W + B

    # Store - compiler handles coalescing
    ct.store(Y, index=(bid, 0), tile=y)
```

**Benefits**:
- âœ… **87% code reduction** (150 â†’ 20 lines)
- âœ… **No manual synchronization** - compiler infers dependencies
- âœ… **Fewer bugs** - no thread indexing errors
- âœ… **Better performance** - compiler sees high-level intent

### Tile-Based Programming with cutile

cutile (cuda.tile) is NVIDIA's framework for writing high-performance GPU kernels using tile-based abstractions:

```python
import cuda.tile as ct

@ct.kernel(num_ctas=ct.ByTarget(sm_100=2, sm_120=1), occupancy=4)
def matmul_kernel(A, B, C, TM: ct.Constant[int], TN: ct.Constant[int], TK: ct.Constant[int]):
    # Get tile indices with 2D swizzle
    bid_m, bid_n = swizzle_2d(M, N, TM, TN)

    # Accumulator in fp32 for precision
    acc = ct.full((TM, TN), 0, dtype=ct.float32)

    # K-loop with TMA and TF32
    for k in range(num_tiles_k):
        # Load tiles from global memory (async with TMA)
        a = ct.load(A, index=(bid_m, k), shape=(TM, TK),
                    padding_mode=ct.PaddingMode.ZERO, latency=4, allow_tma=True)
        b = ct.load(B, index=(k, bid_n), shape=(TK, TN),
                    padding_mode=ct.PaddingMode.ZERO, latency=4, allow_tma=True)

        # Tile-based matrix multiply (uses Tensor Cores)
        dtype = ct.tfloat32 if A.dtype == ct.float32 else A.dtype
        acc = ct.mma(a.astype(dtype), b.astype(dtype), acc)

    # Store result back to global memory
    ct.store(C, index=(bid_m, bid_n), tile=acc.astype(C.dtype))
```

**Key Concepts**:
- **Tiles**: Fixed-size blocks of data (must be power-of-2)
- **TMA**: Hardware async copy from global â†’ shared memory
- **`ct.mma()`**: Automatic Tensor Core dispatch
- **Memory Hierarchy**: Explicit control over register/shared/global

### Flash Attention Implementation

Our Flash Attention kernel implements the online softmax algorithm:

1. **Tiled QK^T**: Compute attention scores in tiles
2. **Online Softmax**: Update running max and sum without materializing full matrix
3. **Weighted Sum**: Compute output as running weighted sum

**Memory Savings**:
- Standard attention: O(NÂ²) memory for full attention matrix
- Flash attention: O(N) memory (only tiles in registers/shared)

**Reference**: [Flash Attention Paper](https://arxiv.org/abs/2205.14135)

---

## ğŸ“ˆ Optimization Journey

See [`OPTIMIZATION_SUMMARY.md`](OPTIMIZATION_SUMMARY.md) for detailed breakdown of:
- Initial bottleneck analysis
- Fused MLP removal (eliminated 14x slowdown)
- Weight transpose caching (28% speedup)
- Performance progression to PyTorch parity

---

## ğŸ›£ï¸ Roadmap

### Completed âœ…
- [x] **Tile Programming Philosophy** - Complete declarative GPT implementation
- [x] **41x GELU speedup** - Compiler-optimized kernels
- [x] **PyTorch parity** - Achieved 1.01x speedup on full model
- [x] **Weight transpose caching** - 28% average improvement
- [x] **Flash Attention** - O(N) memory online softmax
- [x] **Interactive dashboard** - Plotly-based visualization
- [x] **NVIDIA profiling** - nsys/ncu integration
- [x] **Complete demo** - `demo_tile_gpt.py` with all tests passing

### In Progress ğŸš§
- [ ] **MLIR backend research** - Compile-time optimization (optional)
- [ ] **Educational content** - Tile Programming tutorials

### Future Work ğŸ”®
- [ ] **FP16/BF16 support** - Mixed precision for 2-3x speedup
- [ ] **KV cache** for generation (reduce recomputation)
- [ ] **Multi-GPU support** via CuPy's NCCL integration
- [ ] **INT8 quantization** kernels for Hopper Tensor Cores
- [ ] **Auto-tuning system** - Automatic tile size selection
- [ ] **Kernel fusion** - Compiler-driven operation fusion

---

## ğŸ“š API Reference

### `CutileGPTConfig`

Configuration dataclass for model architecture.

**Factory Methods**:
- `gpt_nano()` - 3 layers, 48 dims, 3 heads
- `gpt_micro()` - 4 layers, 128 dims, 4 heads
- `gpt_mini()` - 6 layers, 192 dims, 6 heads
- `gpt2()` - 12 layers, 768 dims, 12 heads (124M params)
- `gpt_tile_small()` - 4 layers, **64 dims**, 4 heads (power-of-2)
- `gpt_tile_medium()` - 6 layers, **128 dims**, 4 heads (power-of-2)
- `gpt_tile_large()` - 8 layers, **256 dims**, 8 heads (power-of-2)

### `CutileGPT`

Main model class for inference.

**Constructor**:
```python
CutileGPT(config: CutileGPTConfig, device: str = 'cuda')
```

**Methods**:
- `forward(idx: cp.ndarray) -> Tuple[cp.ndarray, None]`
  - **Input**: Token indices `(batch, seq_len)` as CuPy array
  - **Output**: Logits `(batch, seq_len, vocab_size)`

- `generate(idx: cp.ndarray, max_new_tokens: int, temperature: float = 1.0, top_k: Optional[int] = None) -> cp.ndarray`
  - Autoregressive generation
  - Returns extended sequence `(batch, seq_len + max_new_tokens)`

- `load_from_mingpt(mingpt_model)`
  - Load pretrained weights from PyTorch minGPT model
  - Converts tensors to CuPy arrays via NumPy bridge

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. **Format**: Use `ruff format` for code formatting
2. **Linting**: Run `ruff check` before committing
3. **Testing**: Ensure `compare.py` passes correctness tests
4. **Kernels**: Maintain cutile best practices (power-of-2 tiles, explicit shapes)

---

## ğŸ“ What We've Proven

cutileGPT demonstrates that **Tile Programming Philosophy** is not just theoretical - it's practical:

### âœ… Declarative GPU Programming Works
- Complete GPT model with ZERO explicit thread management
- Every operation specifies WHAT, compiler handles HOW
- No manual synchronization anywhere in the codebase

### âœ… Performance is Competitive
- **41x speedup** on GELU kernel vs CuPy
- **Matches PyTorch** on full model (1.01x)
- Compiler optimization is effective and automatic

### âœ… Code is Maintainable
- **87% code reduction** vs traditional CUDA
- Readable and clear algorithmic intent
- Easy to modify and extend

### âœ… The Future of GPU Programming
- **Declarative > Imperative** - Higher abstraction level
- **Compiler > Manual** - Better optimization
- **Portable > Specific** - Hardware-independent code

## ğŸ“– Citation

If you use cutileGPT in your research or project, please cite:

```bibtex
@software{cutilegpt2026,
  title={cutileGPT: Tile Programming Philosophy for GPT},
  subtitle={Declarative GPU Programming with NVIDIA CUDA Tile},
  author={Falcon Eyes},
  year={2026},
  url={https://github.com/falcons-eyes/cutileGPT},
  note={41x kernel speedup, PyTorch parity, 87% code reduction}
}
```

---

## ğŸ“„ License

Apache-2.0

This project is licensed under the Apache License 2.0. See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- **NVIDIA CUDA Tile**: Declarative GPU programming framework ([cuda-tile-python](https://github.com/NVIDIA/cuda-tile-python))
- **Andrej Karpathy's minGPT**: Reference architecture ([minGPT](https://github.com/karpathy/minGPT))
- **CuPy**: NumPy-compatible GPU arrays ([CuPy](https://cupy.dev/))
- **Flash Attention**: Online softmax algorithm ([Dao et al., 2022](https://arxiv.org/abs/2205.14135))

## ğŸ”— Learn More

- ğŸ“– [TILE_PHILOSOPHY_DEMO.md](TILE_PHILOSOPHY_DEMO.md) - Complete philosophy documentation
- ğŸ—ï¸ [ARCHITECTURE_VISION.md](ARCHITECTURE_VISION.md) - Project vision & two-path approach
- ğŸ”¬ [CUTILE_PYTHON_PHILOSOPHY_ANALYSIS.md](CUTILE_PYTHON_PHILOSOPHY_ANALYSIS.md) - Deep analysis
- ğŸ® [demo_tile_gpt.py](demo_tile_gpt.py) - Run the complete demo!

---

<div align="center">

**Built with ğŸ’š using Tile Programming Philosophy**

*Think in WHAT (operations), not HOW (threads)*

### This is the future of GPU programming ğŸš€

</div>
