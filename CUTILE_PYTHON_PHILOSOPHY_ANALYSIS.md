# CUDA Tile Python: True Tile Philosophy Analysis

## ðŸŽ¯ í•µì‹¬ ì§ˆë¬¸
**CUDA Tile Pythonë§Œìœ¼ë¡œë„ ì¶©ë¶„ížˆ Tile Programming ì² í•™ì„ ë”°ë¥¼ ìˆ˜ ìžˆëŠ”ê°€?**
**MLIRì´ ì •ë§ í•„ìš”í•œê°€?**

---

## ðŸ“š ë¬¸ì„œ ë¶„ì„ ê²°ê³¼

### 1. Execution Model

#### íŠ¹ì§•
```python
@cuda.tile.kernel
def my_kernel(arr):
    # Block-level parallelismë§Œ ëª…ì‹œ
    # Thread-levelì€ ì™„ì „ížˆ ì¶”ìƒí™”ë¨
    tile = cuda.tile.load(arr)
    result = cuda.tile.sum(tile)  # ìžë™ìœ¼ë¡œ ë³‘ë ¬ ì‹¤í–‰
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
- âœ… **Thread ì¶”ìƒí™”**: "Threads cannot be explicitly identified or manipulated"
- âœ… **ì„ ì–¸ì **: Block-levelë§Œ ëª…ì‹œ, thread-levelì€ í•˜ë“œì›¨ì–´ê°€ ê²°ì •
- âœ… **ìžë™ ë³‘ë ¬í™”**: "Array operations are collectively executed in parallel"
- âœ… **ë™ê¸°í™” ë¶ˆí•„ìš”**: "Explicit synchronization within a block is not permitted"

**ë¹„êµ:**
```cuda
// ì „í†µì  CUDA - ëª…ì‹œì  thread ê´€ë¦¬
__global__ void kernel() {
    int tid = threadIdx.x;  // âŒ ëª…ì‹œì  thread ID
    __shared__ float smem[256];  // âŒ ëª…ì‹œì  shared memory
    __syncthreads();  // âŒ ëª…ì‹œì  ë™ê¸°í™”
}

// CUDA Tile Python - ì¶”ìƒí™”ë¨
@cuda.tile.kernel
def kernel(arr):
    tile = cuda.tile.load(arr)  # âœ… ìžë™ ë³‘ë ¬ ë¡œë“œ
    result = cuda.tile.sum(tile)  # âœ… ìžë™ reduction
```

---

### 2. Data Model

#### Tileì˜ ë³¸ì§ˆ
```python
# Tileì€ ë¶ˆë³€ (immutable)
tile = cuda.tile.load(arr)  # ìƒì„±
tile2 = cuda.tile.add(tile, 1)  # ìƒˆ tile ë°˜í™˜, ì›ë³¸ ë¶ˆë³€

# ë©”ëª¨ë¦¬ì— ì‹¤ì œë¡œ ì¡´ìž¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìžˆìŒ
# - ë ˆì§€ìŠ¤í„°ì—ë§Œ ì¡´ìž¬
# - ì»´íŒŒì¼ëŸ¬ê°€ ìµœì í™”
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
- âœ… **ë¶ˆë³€ì„±**: Functional programming ìŠ¤íƒ€ì¼
- âœ… **ë©”ëª¨ë¦¬ ì¶”ìƒí™”**: "Don't necessarily exist in memory"
- âœ… **í¬ì¸í„° ì—†ìŒ**: "Deliberately avoids exposing pointers"
- âœ… **NumPy ì˜ë¯¸ë¡ **: Broadcasting, shape ì—°ì‚°

**ë¹„êµ:**
```python
# PyTorch - mutable tensors
x = torch.tensor([1, 2, 3])
x += 1  # âŒ In-place ìˆ˜ì •

# CUDA Tile - immutable tiles
tile = cuda.tile.full((256,), 1.0)
tile2 = cuda.tile.add(tile, 1)  # âœ… ìƒˆ tile ìƒì„±
```

---

### 3. Memory Model

#### ì¶”ìƒí™” ìˆ˜ì¤€
```python
# ì‚¬ìš©ìžëŠ” ë©”ëª¨ë¦¬ ê³„ì¸µ ì‹ ê²½ ì•ˆ ì”€
tile = cuda.tile.load(arr)  # Global? Shared? ì»´íŒŒì¼ëŸ¬ê°€ ê²°ì •

# Optional control
tile = cuda.tile.load(arr, order='F')  # ì›í•˜ë©´ layout ì§€ì • ê°€ëŠ¥
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
- âœ… **ë†’ì€ ì¶”ìƒí™”**: "High level of abstraction from hardware"
- âœ… **ìžë™ reordering**: "Compiler and hardware to reorder operations"
- âœ… **Optional control**: Layoutì€ ì„ íƒì ìœ¼ë¡œ ì§€ì •
- âœ… **ê³„ì¸µ ìˆ¨ê¹€**: Shared/global memory êµ¬ë¶„ ì—†ìŒ

**ë¹„êµ:**
```cuda
// CUDA - ëª…ì‹œì  ë©”ëª¨ë¦¬ ê´€ë¦¬
__global__ void kernel() {
    __shared__ float smem[256];  // âŒ Shared memory ëª…ì‹œ
    float val = input[tid];       // âŒ Global load ëª…ì‹œ
    smem[threadIdx.x] = val;      // âŒ Shared store ëª…ì‹œ
    __syncthreads();              // âŒ ë™ê¸°í™” ëª…ì‹œ
}

// CUDA Tile Python - ìžë™ ê´€ë¦¬
@cuda.tile.kernel
def kernel(arr):
    tile = cuda.tile.load(arr)  # âœ… ì»´íŒŒì¼ëŸ¬ê°€ ë©”ëª¨ë¦¬ ê³„ì¸µ ê²°ì •
    result = cuda.tile.sum(tile)  # âœ… ìµœì  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
```

---

### 4. Operations

#### ê³ ìˆ˜ì¤€ ì—°ì‚°
```python
# 12ê°œ ì¹´í…Œê³ ë¦¬ì˜ ì—°ì‚° ì œê³µ

# Reduction - ì„ ì–¸ì !
result = cuda.tile.sum(tile, axis=0)  # ì–´ë–»ê²Œ reduce? ì»´íŒŒì¼ëŸ¬ê°€ ê²°ì •

# Broadcasting - NumPy ìŠ¤íƒ€ì¼
broadcasted = cuda.tile.broadcast_to(tile, (256, 256))

# Matrix multiply - í•˜ë“œì›¨ì–´ ê°€ì† ìžë™
result = cuda.tile.matmul(a, b)  # Tensor Core ìžë™ ì‚¬ìš©
```

**ì œê³µë˜ëŠ” ì—°ì‚°:**
1. **Load/Store**: `load`, `store`, `gather`, `scatter`
2. **Factory**: `arange`, `full`, `ones`, `zeros`
3. **Shape**: `cat`, `broadcast_to`, `reshape`, `permute`
4. **Reduction**: `sum`, `max`, `min`, `prod`, `argmax`, `argmin`
5. **Scan**: `cumsum`, `cumprod`
6. **Matmul**: `mma`, `matmul`
7. **Selection**: `where`, `extract`
8. **Math**: `add`, `sub`, `mul`, `div`, `exp`, `sin`, `sqrt`...
9. **Bitwise**: AND, OR, XOR, shifts
10. **Comparison**: `>`, `==`, `<`...
11. **Atomic**: CAS, exchange, atomic ops
12. **Utility**: `printf`, `assert_`

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
- âœ… **ì„ ì–¸ì  API**: ë¬´ì—‡ì„ í•˜ê³  ì‹¶ì€ì§€ë§Œ ëª…ì‹œ
- âœ… **ê³ ìˆ˜ì¤€ ì¶”ìƒí™”**: Thread/block ê´€ë¦¬ ìˆ¨ê¹€
- âœ… **NumPy ì˜ë¯¸ë¡ **: Broadcasting ê·œì¹™ ë™ì¼
- âœ… **í•˜ë“œì›¨ì–´ ìµœì í™”**: Tensor Core ìžë™ í™œìš©

---

### 5. Performance

#### ìžë™ vs ìˆ˜ë™ ìµœì í™”
```python
# ì™„ì „ ìžë™ - ížŒíŠ¸ ì—†ì´ë„ ìž‘ë™
@cuda.tile.kernel
def kernel(arr):
    tile = cuda.tile.load(arr)  # ì»´íŒŒì¼ëŸ¬ê°€ latency ì¶”ë¡ 
    return cuda.tile.sum(tile)

# ì„ íƒì  íŠœë‹ - ì›í•˜ë©´ ížŒíŠ¸ ì œê³µ
@cuda.tile.kernel
def kernel(arr):
    tile = cuda.tile.load(arr, latency=5)  # DRAM íŠ¸ëž˜í”½ ížŒíŠ¸
    return cuda.tile.sum(tile)

# ì•„í‚¤í…ì²˜ë³„ ì„¤ì •
kernel.configure(
    num_ctas=cuda.tile.ByTarget({90: 4, 89: 2}),
    occupancy=cuda.tile.ByTarget({90: 2, 89: 1})
)
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
- âœ… **ê¸°ë³¸ ìžë™í™”**: "Compiler will infer the latency"
- âœ… **Optional hints**: "Kernels will compile and run without specifying them"
- âœ… **ì‚¬ìš©ìž ì œì–´**: ì›í•˜ë©´ ì„¸ë°€í•œ íŠœë‹ ê°€ëŠ¥
- âœ… **ì ì§„ì  ìµœì í™”**: ê¸°ë³¸ë¶€í„° ì‹œìž‘, í•„ìš”ì‹œ íŠœë‹

---

### 6. Debugging

#### ê³ ìˆ˜ì¤€ ì¶”ìƒí™” ìœ ì§€
```python
# Python-level exceptions
try:
    result = my_kernel(arr)
except cuda.tile.TileSyntaxError:
    print("Syntax error in tile code")
except cuda.tile.TileTypeError:
    print("Type mismatch")

# IR ì¶œë ¥ìœ¼ë¡œ ë””ë²„ê¹…
# CUDA_TILE_LOGS=CUTILEIR python script.py
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
- âœ… **Python-level errors**: Low-level ë””í…Œì¼ ìˆ¨ê¹€
- âœ… **ê³ ìˆ˜ì¤€ ì¶”ìƒí™”**: TileSyntaxError, TileTypeError
- âœ… **ì„ íƒì  ê¹Šì´**: IR ì¶œë ¥ ê°€ëŠ¥í•˜ì§€ë§Œ optional

---

## ðŸŽ¯ ê²°ë¡ : CUDA Tile Pythonì˜ ì² í•™ í‰ê°€

### âœ… Tile Programming ì² í•™ì„ ì™„ë²½ížˆ ë”°ë¦„

| ì² í•™ ì›ì¹™ | CUDA Tile Python | ì „í†µì  CUDA |
|---------|-----------------|-----------|
| **ì„ ì–¸ì ** | âœ… WHATë§Œ ëª…ì‹œ | âŒ HOW ëª…ì‹œ |
| **ì¶”ìƒí™”** | âœ… Thread/memory ìˆ¨ê¹€ | âŒ ëª¨ë‘ ë…¸ì¶œ |
| **ë¶ˆë³€ì„±** | âœ… Immutable tiles | âŒ Mutable state |
| **ê³ ìˆ˜ì¤€ ì—°ì‚°** | âœ… reduce, broadcast | âŒ ìˆ˜ë™ loop |
| **ìžë™ ìµœì í™”** | âœ… ì»´íŒŒì¼ëŸ¬ ì¶”ë¡  | âŒ ìˆ˜ë™ íŠœë‹ |
| **í¬ì¸í„° ì•ˆì „ì„±** | âœ… í¬ì¸í„° ì—†ìŒ | âŒ í¬ì¸í„° everywhere |

### ðŸ“Š ë¹„êµ: PTX vs Torch vs CUDA Tile Python

```python
# ==========================================
# PTX/CUDA Style (Imperative, Low-level)
# ==========================================
__global__ void layernorm(float* x, float* y, int N) {
    int tid = threadIdx.x;
    __shared__ float smem[256];

    // ëª…ì‹œì  ë¡œë“œ
    float val = x[tid];
    smem[tid] = val;
    __syncthreads();

    // ìˆ˜ë™ reduction
    for (int s = 128; s > 0; s >>= 1) {
        if (tid < s) smem[tid] += smem[tid + s];
        __syncthreads();
    }

    // ìˆ˜ë™ ê³„ì‚°
    float mean = smem[0] / N;
    float diff = val - mean;
    // ... ë” ë§Žì€ ìˆ˜ë™ ìž‘ì—…
}

# ==========================================
# PyTorch Style (Tensor operations)
# ==========================================
def layernorm(x):
    # Tensor ì—°ì‚° - ì—¬ì „ížˆ ì•Œê³ ë¦¬ì¦˜ ëª…ì‹œ
    mean = x.mean(dim=-1, keepdim=True)
    var = x.var(dim=-1, keepdim=True)
    x_norm = (x - mean) / torch.sqrt(var + 1e-5)
    return gamma * x_norm + beta

# âš ï¸ PyTorchëŠ”:
# - ê³ ìˆ˜ì¤€ì´ì§€ë§Œ ì—¬ì „ížˆ "ì–´ë–»ê²Œ" ê³„ì‚°í• ì§€ ëª…ì‹œ
# - GPU ìµœì í™”ëŠ” í”„ë ˆìž„ì›Œí¬ ë‚´ë¶€ì—ì„œ
# - ìƒˆë¡œìš´ ì—°ì‚° ì¶”ê°€ ì–´ë ¤ì›€

# ==========================================
# CUDA Tile Python Style (Declarative, Tile-based)
# ==========================================
@cuda.tile.kernel
def layernorm(x_arr, gamma_arr, beta_arr, y_arr):
    # Tile ë¡œë“œ - ì–´ë–»ê²Œ? ì»´íŒŒì¼ëŸ¬ê°€ ê²°ì •
    x = cuda.tile.load(x_arr)

    # Reduction - ìµœì  ì•Œê³ ë¦¬ì¦˜ì€ ì»´íŒŒì¼ëŸ¬ê°€ ì„ íƒ
    sum_val = cuda.tile.sum(x)
    mean = sum_val / x.shape[0]

    # Broadcasting - ìžë™
    mean_bc = cuda.tile.broadcast_to(mean, x.shape)

    # Element-wise ops - ë³‘ë ¬í™” ìžë™
    x_centered = cuda.tile.sub(x, mean_bc)

    # ë¶„ì‚° ê³„ì‚° - ì„ ì–¸ì 
    sq = cuda.tile.mul(x_centered, x_centered)
    var = cuda.tile.sum(sq) / x.shape[0]
    std = cuda.tile.sqrt(var + 1e-5)

    # ì •ê·œí™”
    std_bc = cuda.tile.broadcast_to(std, x.shape)
    x_norm = cuda.tile.div(x_centered, std_bc)

    # Affine transform
    gamma = cuda.tile.load(gamma_arr)
    beta = cuda.tile.load(beta_arr)
    y = cuda.tile.add(cuda.tile.mul(x_norm, gamma), beta)

    # Store - ì–´ë–»ê²Œ? ì»´íŒŒì¼ëŸ¬ê°€ ê²°ì •
    cuda.tile.store(y_arr, y)

# âœ… CUDA Tile Python:
# - WHATì„ í•˜ê³  ì‹¶ì€ì§€ë§Œ ëª…ì‹œ
# - ì»´íŒŒì¼ëŸ¬ê°€ HOW ìµœì í™”
# - Thread/memory ê´€ë¦¬ ìžë™
```

---

## ðŸ’¡ MLIRì´ í•„ìš”í•œê°€?

### CUDA Tile Pythonìœ¼ë¡œ ì¶©ë¶„í•œ ê²ƒë“¤ âœ…

1. **Tile Programming ì² í•™**: âœ… ì™„ë²½ížˆ ë”°ë¦„
2. **ì„ ì–¸ì  í”„ë¡œê·¸ëž˜ë°**: âœ… WHATë§Œ ëª…ì‹œ
3. **ìžë™ ìµœì í™”**: âœ… ì»´íŒŒì¼ëŸ¬ ì¶”ë¡ 
4. **ê³ ìˆ˜ì¤€ ì¶”ìƒí™”**: âœ… Thread/memory ìˆ¨ê¹€
5. **í¬ì¸í„° ì•ˆì „ì„±**: âœ… í¬ì¸í„° ì—†ìŒ
6. **ê³ ìˆ˜ì¤€ ì—°ì‚°**: âœ… reduce, broadcast ë“±

### MLIRì´ ì¶”ê°€ë¡œ ì œê³µí•˜ëŠ” ê²ƒ ðŸŽ¯

1. **ì»´íŒŒì¼ íƒ€ìž„ ìµœì í™”**
   ```
   CUDA Tile Python: Runtime compilation (JIT)
   MLIR: Compile-time optimization (AOT)
   ```

2. **í•˜ë“œì›¨ì–´ ì´ì‹ì„±**
   ```
   CUDA Tile Python: NVIDIA GPU only
   MLIR: ë‹¤ì–‘í•œ ë°±ì—”ë“œ ê°€ëŠ¥ (NVIDIA, AMD, Intel, CPU...)
   ```

3. **Cross-kernel ìµœì í™”**
   ```
   CUDA Tile Python: ê° ì»¤ë„ ë…ë¦½ ìµœì í™”
   MLIR: ì—¬ëŸ¬ ì»¤ë„ fusion, ì „ì—­ ìµœì í™”
   ```

4. **ìžë™ íŠœë‹**
   ```
   CUDA Tile Python: ìˆ˜ë™ ížŒíŠ¸ ì œê³µ
   MLIR: ìžë™ search space íƒìƒ‰ ê°€ëŠ¥
   ```

5. **ì •ì  ë¶„ì„**
   ```
   CUDA Tile Python: Runtime errors
   MLIR: Compile-time verification
   ```

---

## ðŸ“Š ìµœì¢… í‰ê°€

### cutile_gptì˜ í˜„ìž¬ êµ¬í˜„

```python
# cutile_gpt/kernels/layernorm.py
import tile as ct

@ct.kernel
def layernorm_kernel(x_ptr, gamma_ptr, beta_ptr, y_ptr, n_embd, eps):
    # ì´ë¯¸ Tile Philosophyë¥¼ ìž˜ ë”°ë¥´ê³  ìžˆìŒ!
    pid = ct.program_id(0)
    offsets = ct.arange(0, 256)
    mask = offsets < n_embd

    # ê³ ìˆ˜ì¤€ tile ì—°ì‚°
    x = ct.load(x_ptr + offsets, mask=mask)
    mean = ct.sum(x) / n_embd
    x_centered = x - mean
    # ...
```

**í‰ê°€**: âœ… **ì´ë¯¸ Tile Programming ì² í•™ì„ ë”°ë¥´ê³  ìžˆìŠµë‹ˆë‹¤!**

### 2ê°€ì§€ ê²½ë¡œì˜ ìž¬í‰ê°€

| ì¸¡ë©´ | Path 1 (Python) | Path 2 (MLIR) |
|-----|----------------|---------------|
| **Tile ì² í•™** | âœ… ì™„ë²½ížˆ ë”°ë¦„ | âœ… ì™„ë²½ížˆ ë”°ë¦„ |
| **ì„ ì–¸ì ** | âœ… ì„ ì–¸ì  | âœ… ì„ ì–¸ì  |
| **ì¶”ìƒí™”** | âœ… ë†’ì€ ì¶”ìƒí™” | âœ… ë†’ì€ ì¶”ìƒí™” |
| **ì»´íŒŒì¼ ìµœì í™”** | âš ï¸  JIT (runtime) | âœ… AOT (compile-time) |
| **ì´ì‹ì„±** | âš ï¸  NVIDIA only | âœ… Multiple targets |
| **ê°œë°œ ì†ë„** | âœ… ë¹ ë¦„ | âš ï¸  ëŠë¦¼ |
| **ë””ë²„ê¹…** | âœ… ì‰¬ì›€ | âš ï¸  ì–´ë ¤ì›€ |
| **í•™ìŠµ ê³¡ì„ ** | âœ… Python ì¹œìˆ™ | âš ï¸  MLIR í•™ìŠµ í•„ìš” |

---

## ðŸŽ¯ ê¶Œìž¥ ì‚¬í•­

### Option 1: Python API Focus (ì¶”ì²œ) â­
**CUDA Tile Pythonë§Œìœ¼ë¡œë„ ì¶©ë¶„ížˆ Tile Philosophyë¥¼ êµ¬í˜„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤!**

```python
# cutile_gpt/ (í˜„ìž¬ êµ¬í˜„)
# - ì´ë¯¸ Tile Programming ì² í•™ì„ ë”°ë¦„
# - ì„ ì–¸ì , ê³ ìˆ˜ì¤€, ìžë™ ìµœì í™”
# - ë¹ ë¥¸ ê°œë°œ, ì‰¬ìš´ ë””ë²„ê¹…
# - NVIDIA GPUì— ìµœì í™”

# ê°œì„  ë°©í–¥:
# 1. ë” ë§Žì€ ì»¤ë„ì„ Tile ìŠ¤íƒ€ì¼ë¡œ ìž‘ì„±
# 2. Performance ë²¤ì¹˜ë§ˆí¬
# 3. íŠœë‹ ížŒíŠ¸ í™œìš©
```

### Option 2: Hybrid Approach
**Pythonìœ¼ë¡œ í”„ë¡œí† íƒ€ìž…, MLIRë¡œ ìµœì í™”**

```
ê°œë°œ ë‹¨ê³„: Python API (ë¹ ë¥¸ iteration)
    â†“
ê²€ì¦ ë‹¨ê³„: Performance profiling
    â†“
ìµœì í™” ë‹¨ê³„: MLIRë¡œ critical path ìž¬ìž‘ì„±
    â†“
ë°°í¬ ë‹¨ê³„: í˜¼í•© (ëŒ€ë¶€ë¶„ Python, ë³‘ëª©ë§Œ MLIR)
```

### Option 3: MLIR Focus
**ì—°êµ¬ í”„ë¡œì íŠ¸ë¡œì„œ ê°€ì¹˜ ìžˆì§€ë§Œ ì‹¤ìš©ì„±ì€...**

**ìž¥ì **:
- í•™ìˆ ì  ê¸°ì—¬
- í•˜ë“œì›¨ì–´ ì´ì‹ì„±
- ê³ ê¸‰ ìµœì í™”

**ë‹¨ì **:
- ê°œë°œ ì‹œê°„ â†‘â†‘â†‘
- ë³µìž¡ë„ â†‘â†‘â†‘
- ë””ë²„ê¹… ì–´ë ¤ì›€
- Python APIë¡œë„ ì¶©ë¶„í•œ ì„±ëŠ¥

---

## ðŸš€ ìƒˆë¡œìš´ ë¹„ì „

### cutileGPTì˜ ì§„ì •í•œ ê°€ì¹˜

**"PyTorch ì—†ì´ Tile Programmingìœ¼ë¡œ GPT êµ¬í˜„"**

```
âœ… í•µì‹¬: Python APIë¡œ ì´ë¯¸ ë‹¬ì„± ê°€ëŠ¥!

cutileGPTì˜ contribution:
1. Tile Programming ì² í•™ ì‹œì—°
2. Python APIë¡œ production-ready GPT
3. êµìœ¡ì  ê°€ì¹˜: PTX vs Torch vs Tile ë¹„êµ
4. NVIDIA GPU ìµœì í™” ì»¤ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬
```

### ìˆ˜ì •ëœ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
cutileGPT/
â”‚
â”œâ”€â”€ cutile_gpt/                # Main implementation â­
â”‚   â”œâ”€â”€ model.py              # âœ… Tile-based GPT
â”‚   â”œâ”€â”€ kernels/
â”‚   â”‚   â”œâ”€â”€ layernorm.py     # âœ… Declarative tile ops
â”‚   â”‚   â”œâ”€â”€ attention.py     # âœ… High-level attention
â”‚   â”‚   â”œâ”€â”€ linear.py        # âœ… Tile-based matmul
â”‚   â”‚   â””â”€â”€ gelu.py          # âœ… Tile activation
â”‚   â””â”€â”€ inference.py         # âœ… End-to-end pipeline
â”‚
â”œâ”€â”€ benchmarks/               # NEW: Performance analysis
â”‚   â”œâ”€â”€ vs_pytorch.py        # cutile_gpt vs PyTorch
â”‚   â”œâ”€â”€ vs_cuda.py           # vs hand-written CUDA
â”‚   â””â”€â”€ ablation.py          # Tile size, hints tuning
â”‚
â”œâ”€â”€ examples/                 # NEW: Educational examples
â”‚   â”œâ”€â”€ 01_tile_basics.py    # Tile philosophy intro
â”‚   â”œâ”€â”€ 02_reduce.py         # Declarative reduction
â”‚   â”œâ”€â”€ 03_broadcast.py      # Broadcasting semantics
â”‚   â””â”€â”€ 04_layernorm.py      # Full kernel walkthrough
â”‚
â””â”€â”€ cutile_gpt_mlir/         # Optional research path
    â””â”€â”€ (experimental)
```

---

## ðŸ“š ê²°ë¡ 

### í•µì‹¬ ë°œê²¬

**CUDA Tile Python APIëŠ” ì´ë¯¸ ì§„ì •í•œ Tile Programming ì² í•™ì„ ë”°ë¦…ë‹ˆë‹¤!**

1. âœ… **ì„ ì–¸ì **: WHATë§Œ ëª…ì‹œ
2. âœ… **ê³ ìˆ˜ì¤€ ì¶”ìƒí™”**: Thread/memory ìˆ¨ê¹€
3. âœ… **ë¶ˆë³€ ë°ì´í„°**: Functional style
4. âœ… **ìžë™ ìµœì í™”**: ì»´íŒŒì¼ëŸ¬ ì¶”ë¡ 
5. âœ… **ê³ ìˆ˜ì¤€ ì—°ì‚°**: reduce, broadcast, matmul

### MLIR vs Python API

**MLIRì´ ì œê³µí•˜ëŠ” ì¶”ê°€ ê°€ì¹˜**:
- ì»´íŒŒì¼ íƒ€ìž„ ìµœì í™” (vs JIT)
- í•˜ë“œì›¨ì–´ ì´ì‹ì„± (vs NVIDIA only)
- Cross-kernel fusion
- ìžë™ íŠœë‹

**í•˜ì§€ë§Œ**:
- Python APIë§Œìœ¼ë¡œë„ Tile PhilosophyëŠ” ì™„ë²½ížˆ êµ¬í˜„ë¨
- ê°œë°œ ì†ë„ì™€ ì‹¤ìš©ì„±ì€ Pythonì´ í›¨ì”¬ ìš°ìˆ˜
- ì„±ëŠ¥ë„ Python APIë¡œ ì¶©ë¶„ížˆ ìµœì í™” ê°€ëŠ¥

### ìµœì¢… ê¶Œìž¥

**Focus on Python API (cutile_gpt/)** â­

MLIRì€ ì„ íƒì  ì—°êµ¬ í”„ë¡œì íŠ¸ë¡œ:
- í•™ìˆ ì  í¥ë¯¸
- í•˜ë“œì›¨ì–´ ì´ì‹ì„±ì´ í•„ìš”í•œ ê²½ìš°
- ê·¹í•œ ìµœì í™”ê°€ í•„ìš”í•œ íŠ¹ì • ì»¤ë„

**cutileGPTì˜ ì§„ì •í•œ ê°€ì¹˜ëŠ” Python APIë¡œ Tile Programmingì„ ë³´ì—¬ì£¼ëŠ” ê²ƒ!**
